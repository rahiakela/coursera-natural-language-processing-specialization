{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-sentence-piece-and-bpe.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN6DC+YBt+/Ea0Wq30fXgEv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/coursera-natural-language-processing-specialization/blob/4-natural-language-processing-with-attention-models/week-3/1_sentence_piece_and_bpe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNxlFZZ0b6cc"
      },
      "source": [
        "# SentencePiece and BPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uwUl8PJb7Qj"
      },
      "source": [
        "## Introduction to Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kka68q8kdr90"
      },
      "source": [
        "In order to process text in neural network models, it is first required to **encode** text as numbers with ids (such as the embedding vectors we've been using in the previous assignments), since the tensor operations act on numbers. Finally, if the output of the network are words, it is required to **decode** the predicted tokens ids back to text.\n",
        "\n",
        "To encode text, the first decision that has to be made is to what level of granularity are we going to consider the text? Because ultimately, from these **tokens**, features are going to be created about them. Many different experiments have been carried out using *words*, *morphological units*, *phonemic units*, *characters*. For example, \n",
        "\n",
        "- Tokens are tricky. (raw text)\n",
        "- Tokens are tricky . ([words](https://arxiv.org/pdf/1301.3781))\n",
        "- Token s _ are _ trick _ y . ([morphemes](https://arxiv.org/pdf/1907.02423.pdf))\n",
        "- t oʊ k ə n z _ ɑː _ ˈt r ɪ k i. ([phonemes](https://www.aclweb.org/anthology/W18-5812.pdf), for STT)\n",
        "- T o k e n s _ a r e _ t r i c k y . ([character](https://www.aclweb.org/anthology/C18-1139/))\n",
        "\n",
        "But how to identify these units, such as words, are largely determined by the language they come from. For example, in many European languages a space is used to separate words, while in some Asian languages there are no spaces between words. Compare English and Mandarin.\n",
        "\n",
        "- Tokens are tricky. (original sentence)\n",
        "- 令牌很棘手 (Mandarin)\n",
        "- Lìng pái hěn jí shǒu (pinyin)\n",
        "- 令牌 很 棘手 (Mandarin with spaces)\n",
        "\n",
        "\n",
        "So, the ability to **tokenize**, i.e. split text into meaningful fundamental units is not always straight-forward.\n",
        "\n",
        "Also, there are practical issues of how large our *vocabulary* of words, `vocab_size`, should be, considering memory limitations vs. coverage. A compromise between the finest-grained models employing characters which can be memory and more computationally efficient *subword* units such as [n-grams](https://arxiv.org/pdf/1712.09405) or larger units need to be made.\n",
        "\n",
        "In [SentencePiece](https://www.aclweb.org/anthology/D18-2012.pdf) unicode characters are grouped together using either a [unigram language model](https://www.aclweb.org/anthology/P18-1007.pdf) (used in this week's assignment) or [BPE](https://arxiv.org/pdf/1508.07909.pdf), **byte-pair encoding**. We will discuss BPE, since BERT and many of its variant uses a modified version of BPE and its pseudocode is easy to implement and understand... hopefully!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQf_lnTxdz2z"
      },
      "source": [
        "## SentencePiece Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M6Wfp1Ud35E"
      },
      "source": [
        "### NFKC Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I12zon2d6Lv"
      },
      "source": [
        "Unsurprisingly, even using unicode to initially tokenize text can be ambiguous, e.g., "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uZui8-Zd9ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3aec5a5-e44a-447b-de9b-b68737859b0e"
      },
      "source": [
        "eaccent = '\\u00E9'\n",
        "e_accent = '\\u0065\\u0301'\n",
        "print(f'{eaccent} = {e_accent} : {eaccent == e_accent}')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "é = é : False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49lxDzX9fWuD"
      },
      "source": [
        "SentencePiece uses the Unicode standard Normalization form, [NFKC](https://en.wikipedia.org/wiki/Unicode_equivalence), so this isn't an issue. Looking at our example from above again with normalization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddu6PMpWfZmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4702b6f4-306e-4db8-ea43-f29965ed6665"
      },
      "source": [
        "from unicodedata import normalize\n",
        "\n",
        "norm_eaccent = normalize(\"NFKC\", \"\\u00E9\")\n",
        "norm_e_accent = normalize(\"NFKC\", \"\\u0065\\u0301\")\n",
        "print(f\"{norm_eaccent} = {norm_e_accent} : {norm_eaccent == norm_e_accent}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "é = é : True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4wr-l5FPO2c"
      },
      "source": [
        "Normalization has actually changed the unicode code point (unicode unique id) for one of these two characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AORzGiOnPPXO"
      },
      "source": [
        "def get_hex_encoding(s):\n",
        "  return \" \".join(hex(ord(c)) for c in s)\n",
        "\n",
        "def print_string_and_encoding(s):\n",
        "  print(f\"{s} : {get_hex_encoding(s)}\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMY-PdvIP0aU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d332de9-4aa1-4814-ecbd-2a2cc613cb08"
      },
      "source": [
        "for s in [eaccent, e_accent, norm_eaccent, norm_e_accent]:\n",
        "  print_string_and_encoding(s)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "é : 0xe9\n",
            "é : 0x65 0x301\n",
            "é : 0xe9\n",
            "é : 0xe9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocPzedrxQvGn"
      },
      "source": [
        "This normalization has other side effects which may be considered useful such as converting curly quotes &ldquo; to \" their ASCII equivalent. (Although we *now* lose directionality of the quote...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtx3JXnIQvv3"
      },
      "source": [
        "### Lossless Tokenization<sup>*</sup>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqmS1a-GQ5hI"
      },
      "source": [
        "SentencePiece also ensures that when you tokenize your data and detokenize your data the original position of white space is preserved. (However, tabs and newlines are converted to spaces, please try this experiment yourself later below.)\n",
        "\n",
        "To ensure this **lossless tokenization** it replaces white space with _ (U+2581). So that a simple join of the replace underscores with spaces can restore the white space, even if there are consecutives symbols. But remember first to normalize and then replace spaces with _ (U+2581). As the following example shows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f79q1MYQsFZ"
      },
      "source": [
        "s = \"Tokenization is hard.\"\n",
        "s_ = s.replace(\" \", \"\\u2581\")\n",
        "s_n = normalize(\"NFKC\", \"Tokenization is hard.\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzjwNGpPUtZ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d2db4f-171c-47c6-b8c8-ca465dea4cef"
      },
      "source": [
        "print(get_hex_encoding(s))\n",
        "print(get_hex_encoding(s_))\n",
        "print(get_hex_encoding(s_n))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n",
            "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x2581 0x69 0x73 0x2581 0x68 0x61 0x72 0x64 0x2e\n",
            "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DNt08gsVGF3"
      },
      "source": [
        "So the special unicode underscore was replaced by the ASCII unicode. Reversing the order, we see that the special unicode underscore was retained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DLbORP6U4YK"
      },
      "source": [
        "s = \"Tokenization is hard.\"\n",
        "sn = normalize(\"NFKC\", \"Tokenization is hard.\")\n",
        "sn_ = s.replace(\" \", \"\\u2581\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSSo7BSHWOVQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec70e8c-bb19-49cf-ab20-979dac97b443"
      },
      "source": [
        "print(get_hex_encoding(s))\n",
        "print(get_hex_encoding(sn))\n",
        "print(get_hex_encoding(sn_))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n",
            "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n",
            "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x2581 0x69 0x73 0x2581 0x68 0x61 0x72 0x64 0x2e\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhteWmUfWYXL"
      },
      "source": [
        "## BPE Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEed2V6SWY6n"
      },
      "source": [
        "Now that we have discussed the preprocessing that SentencePiece performs we will go get our data, preprocess, and apply the BPE algorithm. We will show how this reproduces the tokenization produced by training SentencePiece on our example dataset (from this week's assignment)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nli_bdIWcRB"
      },
      "source": [
        "### Preparing our Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpYKNWNVWeqo"
      },
      "source": [
        "First, we get our Squad data and process as above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT8uIXKMWmP9"
      },
      "source": [
        "import ast\n",
        "\n",
        "def convert_json_examples_to_text(filepath):\n",
        "  example_jsons = list(map(ast.literal_eval, open(filepath)))                        # Read in the json from the example file\n",
        "  texts = [example_json['text'].decode('utf-8') for example_json in example_jsons]   # Decode the byte sequences\n",
        "  text = \"\\n\\n\".join(texts)        # Separate different articles by two newlines\n",
        "  text = normalize(\"NFKC\", text)   # Normalize the text\n",
        "\n",
        "  with open(\"example.txt\", \"w\") as fw:\n",
        "    fw.write(text)\n",
        "\n",
        "  return text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpX4wwFiihnC",
        "outputId": "cbef818d-00cc-43ab-eed2-9717a759f17c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/rahiakela/coursera-natural-language-processing-specialization/4-natural-language-processing-with-attention-models/week-3/data.txt\n",
        "!wget https://raw.githubusercontent.com/rahiakela/coursera-natural-language-processing-specialization/4-natural-language-processing-with-attention-models/week-3/example.txt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-21 06:43:17--  https://raw.githubusercontent.com/rahiakela/coursera-natural-language-processing-specialization/4-natural-language-processing-with-attention-models/week-3/data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5513 (5.4K) [text/plain]\n",
            "Saving to: ‘data.txt.1’\n",
            "\n",
            "\rdata.txt.1            0%[                    ]       0  --.-KB/s               \rdata.txt.1          100%[===================>]   5.38K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-21 06:43:17 (57.7 MB/s) - ‘data.txt.1’ saved [5513/5513]\n",
            "\n",
            "--2020-11-21 06:43:17--  https://raw.githubusercontent.com/rahiakela/coursera-natural-language-processing-specialization/4-natural-language-processing-with-attention-models/week-3/example.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4538 (4.4K) [text/plain]\n",
            "Saving to: ‘example.txt’\n",
            "\n",
            "example.txt         100%[===================>]   4.43K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-21 06:43:18 (54.7 MB/s) - ‘example.txt’ saved [4538/4538]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DddVIzC5WUlq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5d71bd-77ed-466a-9c1d-0612a042ae87"
      },
      "source": [
        "text = convert_json_examples_to_text(\"data.txt\")\n",
        "print(text[:900])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginners BBQ Class Taking Place in Missoula!\n",
            "Do you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\n",
            "He will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\n",
            "The cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.\n",
            "\n",
            "Discussion in 'Mac OS X Lion (10.7)' started by axboi87, Jan 20, 2012.\n",
            "I've got a 500gb internal drive and a 240gb SSD.\n",
            "When trying to restore using di\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2mY8beVZ-N8"
      },
      "source": [
        "In the algorithm the `vocab` variable is actually a frequency dictionary of the words. Further, those words have been prepended with an *underscore* to indicate that they are the beginning of a word. \n",
        "\n",
        "Finally, the characters have been delimited by spaces so that the BPE algorithm can group the most common characters together in the dictionary in a greedy fashion. We will see how that is exactly done shortly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4qCPx5sYNX5"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQYtJ573bu6l"
      },
      "source": [
        "vocab = Counter([\"\\u2581\" + word for word in text.split()])\n",
        "vocab = {\" \".join([l for l in word]): freq for word, freq in vocab.items()}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_R8wJB5cItU"
      },
      "source": [
        "def show_vocab(vocab, end=\"\\n\", limit=20):\n",
        "  shown = 0\n",
        "  for word, freq in vocab.items():\n",
        "    print(f\"{word}: {freq}\", end=end)\n",
        "    shown += 1\n",
        "    if shown > limit:\n",
        "      break"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBVgCa4pcv6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f09237-c33e-416a-9d72-e11527b0541a"
      },
      "source": [
        "show_vocab(vocab)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "▁ B e g i n n e r s: 1\n",
            "▁ B B Q: 3\n",
            "▁ C l a s s: 2\n",
            "▁ T a k i n g: 1\n",
            "▁ P l a c e: 1\n",
            "▁ i n: 15\n",
            "▁ M i s s o u l a !: 1\n",
            "▁ D o: 1\n",
            "▁ y o u: 13\n",
            "▁ w a n t: 1\n",
            "▁ t o: 33\n",
            "▁ g e t: 2\n",
            "▁ b e t t e r: 2\n",
            "▁ a t: 1\n",
            "▁ m a k i n g: 2\n",
            "▁ d e l i c i o u s: 1\n",
            "▁ B B Q ?: 1\n",
            "▁ Y o u: 1\n",
            "▁ w i l l: 6\n",
            "▁ h a v e: 4\n",
            "▁ t h e: 31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3PhPboic4y0"
      },
      "source": [
        "We check the size of the vocabulary (frequency dictionary) because this is the one hyperparameter that BPE depends on crucially on how far it breaks up a word into SentencePieces. \n",
        "\n",
        "It turns out that for our trained model on our small dataset that 60% of 455 merges of the most frequent characters need to be done to reproduce the upperlimit of a 32K `vocab_size` over the entire corpus of examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4H-F5Egcy6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1681e91a-d779-4d76-a1c8-14d65cfcaade"
      },
      "source": [
        "print(f\"Total number of unique words: {len(vocab)}\")\n",
        "print(f\"Number of merges required to reproduce SentencePiece training on the whole corpus: {int(0.60 * len(vocab))}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of unique words: 455\n",
            "Number of merges required to reproduce SentencePiece training on the whole corpus: 273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtId5qHud9kA"
      },
      "source": [
        "### BPE Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt5E9FV5d-Ht"
      },
      "source": [
        "Directly from the BPE paper we have the following algorithm. \n",
        "\n",
        "To understand what's going on first take a look at the third function `get_sentence_piece_vocab`. It takes in the current `vocab` word-frequency dictionary and the fraction of the total `vocab_size` to merge characters in the words of the dictionary, `num_merges` times. Then for each *merge* operation it `get_stats` on how many of each pair of character sequences there are. It gets the most frequent *pair* of symbols as the `best` pair. Then it merges those pair of symbols (removes the space between them) in each word in the `vocab` that contains this `best` (= `pair`). Consquently, `merge_vocab` creates a new `vocab`, `v_out`. This process is repeated `num_merges` times and the result is the set of SentencePieces (keys of the final `sp_vocab`).\n",
        "\n",
        "Please feel free to skip the below if the above description was enough.\n",
        "\n",
        "In a little more detail then, we can see in `get_stats` we initially create a list of bigram frequencies (two character sequence) from our vocabulary. Later, this may include (trigrams, quadgrams, etc.). Note that the key of the `pairs` frequency dictionary is actually a 2-tuple, which is just shorthand notation for a pair.\n",
        "\n",
        "In `merge_vocab` we take in an individual `pair` (of character sequences, note this is the most frequency `best` pair) and the current `vocab` as `v_in`. We create a new `vocab`, `v_out`, from the old by joining together the characters in the pair (removing the space), if they are present in the a word of the dictionary. \n",
        "\n",
        "[Warning](https://regex101.com/): the expression `(?<!\\S)` means that either whitespace character follows before the `bigram` or there is nothing before (beginning of word) the bigram, similarly for `(?!\\S)` for preceding whitespace or end of word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbsJ5isndTjd"
      },
      "source": [
        "import re, collections"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgIVj1G5iXW6"
      },
      "source": [
        "def get_stats(vocab):\n",
        "  pairs = collections.defaultdict(int)\n",
        "  for word, freq in vocab.items():\n",
        "    symbols = word.split()\n",
        "    for i in range(len(symbols) - 1):\n",
        "      pairs[symbols[i], symbols[i+1]] += freq\n",
        "  return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "  v_out = {}\n",
        "  bigram = re.escape(\" \".join(pair))\n",
        "  p = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
        "  for word in v_in:\n",
        "    w_out = p.sub(\"\".join(pair), word)\n",
        "    v_out[w_out] = v_in[word]\n",
        "  return v_out\n",
        "\n",
        "def get_sentence_piece_vocab(vocab, frac_merges=0.60):\n",
        "  sp_vocab = vocab.copy()\n",
        "  num_merges = int(len(sp_vocab) * frac_merges)\n",
        "\n",
        "  for i in range(num_merges):\n",
        "    pairs = get_stats(sp_vocab)\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    sp_vocab = merge_vocab(best, sp_vocab)\n",
        "  \n",
        "  return sp_vocab"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzfqdw2qnvOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "261de5ef-464f-4d3e-ba2a-e0b14854ef24"
      },
      "source": [
        "sp_vocab = get_sentence_piece_vocab(vocab)\n",
        "show_vocab(sp_vocab)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "▁B e g in n ers: 1\n",
            "▁BBQ: 3\n",
            "▁Cl ass: 2\n",
            "▁T ak ing: 1\n",
            "▁P la ce: 1\n",
            "▁in: 15\n",
            "▁M is s ou la !: 1\n",
            "▁D o: 1\n",
            "▁you: 13\n",
            "▁w an t: 1\n",
            "▁to: 33\n",
            "▁g et: 2\n",
            "▁be t ter: 2\n",
            "▁a t: 1\n",
            "▁mak ing: 2\n",
            "▁d e l ic i ou s: 1\n",
            "▁BBQ ?: 1\n",
            "▁ Y ou: 1\n",
            "▁will: 6\n",
            "▁have: 4\n",
            "▁the: 31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdKayvb2oVUw"
      },
      "source": [
        "## Train SentencePiece BPE Tokenizer on Example Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj2AMt9woV2H"
      },
      "source": [
        "### Explore SentencePiece Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_kJfrIUoX76"
      },
      "source": [
        "First let us explore the SentencePiece model provided with this week's assignment. Remember you can always use Python's built in `help` command to see the documentation for any object or method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQdt1a7jl_P3",
        "outputId": "31529a11-f4a4-47a8-b5c9-998f600603a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://github.com/rahiakela/coursera-natural-language-processing-specialization/blob/4-natural-language-processing-with-attention-models/week-3/sentencepiece.model"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-21 06:58:33--  https://github.com/rahiakela/coursera-natural-language-processing-specialization/blob/4-natural-language-processing-with-attention-models/week-3/sentencepiece.model\n",
            "Resolving github.com (github.com)... 52.192.72.89\n",
            "Connecting to github.com (github.com)|52.192.72.89|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘sentencepiece.model’\n",
            "\n",
            "sentencepiece.model     [ <=>                ]  98.20K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-11-21 06:58:34 (868 KB/s) - ‘sentencepiece.model’ saved [100561]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3U_sNA_myRR",
        "outputId": "b8f698c5-126f-4011-9af8-65ddef22d361",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install sentencepiece\n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n",
            "--2020-11-21 07:10:51--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 278779 (272K) [text/plain]\n",
            "Saving to: ‘botchan.txt’\n",
            "\n",
            "botchan.txt         100%[===================>] 272.25K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-11-21 07:10:52 (4.73 MB/s) - ‘botchan.txt’ saved [278779/278779]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gA8eg65n6mc",
        "outputId": "bef347d4-6a71-4eab-9b38-407115a50430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# sp = spm.SentencePieceProcessor(model_file=\"sentencepiece.model\")\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"sentencepiece.model\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-777567d8d6f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# sp = spm.SentencePieceProcessor(model_file=\"sentencepiece.model\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentencepiece.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    365\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mDecodeIdsWithCheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Internal: /sentencepiece/python/bundled/sentencepiece/src/sentencepiece_processor.cc(823) [model_proto->ParseFromArray(serialized.data(), serialized.size())] "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPUQt5Jclg7b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}