{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment-2--transformer-summarizer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO5OFOL+nSrhKg7gRa9/GYH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/coursera-natural-language-processing-specialization/blob/master/course-4-natural-language-processing-with-attention-models/week-2-Text%20Summarization/assignment_2_transformer_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELYyBdfz_z9r"
      },
      "source": [
        "## Assignment 2: Transformer Summarizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we-CD4y-ArpW"
      },
      "source": [
        "Welcome to the second assignment of course 4. In this assignment you will explore summarization using the transformer model. Yes, you will implement the transformer decoder from scratch, but we will slowly walk you through it. There are many hints in this notebook so feel free to use them as needed. \n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deeplearning.ai-NLPS/transformerNews.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUOFZXD8Lasw"
      },
      "source": [
        "## Outline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46eAV3qnLb1-"
      },
      "source": [
        "- [Introduction](#0)\n",
        "- [Part 1: Importing the dataset](#1)\n",
        "    - [1.1 Encode & Decode helper functions](#1.1)\n",
        "    - [1.2 Defining parameters](#1.2)\n",
        "    - [1.3 Exploring the data](#1.3)\n",
        "- [Part 2: Summarization with transformer](#2)\n",
        "    - [2.1 Dot product attention](#2.1)\n",
        "        - [Exercise 01](#ex01)\n",
        "    - [2.2 Causal Attention](#2.2)\n",
        "        - [Exercise 02](#ex02)\n",
        "    - [2.3 Transformer decoder block](#2.3)\n",
        "        - [Exercise 03](#ex03)\n",
        "    - [2.4 Transformer Language model](#2.4)\n",
        "        - [Exercise 04](#ex04)\n",
        "- [Part 3: Training](#3)\n",
        "    - [3.1 Training the model](#3.1)\n",
        "        - [Exercise 05](#ex05)\n",
        "- [Part 4: Evaluation](#4)\n",
        "    - [4.1 Loading in a trained model](#4.1)\n",
        "- [Part 5: Testing with your own input](#5) \n",
        "    - [Exercise 6](#ex06)\n",
        "    - [5.1 Greedy decoding](#5.1)\n",
        "        - [Exercise 07](#ex07)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YXMf-JWLemg"
      },
      "source": [
        "<a name='0'></a>\n",
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYuym2inLjHA"
      },
      "source": [
        "Summarization is an important task in natural language processing and could be useful for a consumer enterprise. For example, bots can be used to scrape articles, summarize them, and then you can use sentiment analysis to identify the sentiment about certain stocks. Anyways who wants to read an article or a long email today, when you can build a transformer to summarize text for you. \n",
        "\n",
        "Let's get started, by completing this assignment you will learn to:  \n",
        "\n",
        "- Use built-in functions to preprocess your data\n",
        "- Implement Dot-Product Attention\n",
        "- Implement Causal Attention\n",
        "- Understand how attention works\n",
        "- Build the transformer model\n",
        "- Evaluate your model\n",
        "- Summarize an article\n",
        "\n",
        "As you can tell, this model is slightly different than the ones you have already implemented. This is heavily based on attention and does not rely on sequences, which allows for parallel computing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDCnUwxALncn"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D8zArgGwST9"
      },
      "source": [
        "!pip -q install trax==1.3.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4fLNMc5LsJ4"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)\n",
        "\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as jnp\n",
        "\n",
        "# to print the entire np array\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROzYQv_rERgR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0dcaea4-1887-460a-e918-33ea6bf3a877"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# wget -q https://github.com/rahiakela/coursera-natural-language-processing-specialization/raw/master/course-4-natural-language-processing-with-attention-models/week-2-Text%20Summarization/data/cnn_dailymail.zip\n",
        "wget -q https://github.com/rahiakela/coursera-natural-language-processing-specialization/raw/master/course-4-natural-language-processing-with-attention-models/week-2-Text%20Summarization/data/vocab_dir/summarize32k.subword.subwords\n",
        "\n",
        "mkdir vocab_dir\n",
        "cp summarize32k.subword.subwords vocab_dir"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ioVVYpTLo7H"
      },
      "source": [
        "<a name='1'></a>\n",
        "## Part 1: Importing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL0y-KzvLyZK"
      },
      "source": [
        "Trax makes it easy to work with Tensorflow's datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5ILG8RcL0nM"
      },
      "source": [
        "# This will download the dataset if no data_dir is specified.\n",
        "# Downloading and processing can take bit of time, so we have the data already in 'data/' for you\n",
        "#!mkdir data\n",
        "\n",
        "# Importing CNN/DailyMail articles dataset\n",
        "train_stream_fn = trax.data.TFDS(\"cnn_dailymail\", data_dir=\"data/\", keys=(\"article\", \"highlights\"), train=True)\n",
        "\n",
        "# This should be much faster as the data is downloaded already.\n",
        "eval_stream_fn = trax.data.TFDS(\"cnn_dailymail\", data_dir=\"data/\", keys=(\"article\", \"highlights\"), train=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6QwT_N0zxCv"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "### 1.1 Tokenize & Detokenize helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7QV5qORzyke"
      },
      "source": [
        "Just like in the previous assignment, the cell above loads in the encoder for you. Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your [Trax](https://github.com/google/trax) models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: \n",
        "\n",
        "- <span style='color:blue'> word2Ind: </span> a dictionary mapping the word to its index.\n",
        "- <span style='color:blue'> ind2Word:</span> a dictionary mapping the index to its word.\n",
        "- <span style='color:blue'> word2Count:</span> a dictionary mapping the word to the number of times it appears. \n",
        "- <span style='color:blue'> num_words:</span> total number of words that have appeared. \n",
        "\n",
        "Since you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:\n",
        "\n",
        "- <span style='color:blue'> tokenize: </span> converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords.\n",
        "- <span style='color:blue'> detokenize: </span> converts a token list to its corresponding sentence (i.e. string)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DCJo2sLzwCs"
      },
      "source": [
        "def tokenize(input_str, EOS=1):\n",
        "  \"\"\"Input str to features dict, ready for inference\"\"\"\n",
        "\n",
        "  # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
        "  # we get around it by making a 1-element stream with `iter`.\n",
        "  inputs = next(trax.data.tokenize(iter([input_str]), vocab_dir=\"vocab_dir/\", vocab_file=\"summarize32k.subword.subwords\"))\n",
        "\n",
        "  # Mark the end of the sentence with EOS\n",
        "  return list(inputs) + [EOS]\n",
        "\n",
        "def detokenize(integers):\n",
        "  \"\"\"List of ints to str\"\"\"\n",
        "\n",
        "  s = trax.data.detokenize(integers, vocab_dir=\"vocab_dir/\", vocab_file=\"summarize32k.subword.subwords\")\n",
        "\n",
        "  return wrapper.fill(s)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCdFkvE30IBp"
      },
      "source": [
        "<a name='1.2'></a>\n",
        "### 1.2 Preprocessing for Language Models: Concatenate It!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSBdhxLt0J_H"
      },
      "source": [
        "This week you will use a language model -- Transformer Decoder -- to solve\n",
        "an input-output problem. As you know, language models only predict the next\n",
        "word, they have no notion of inputs. To create a single input suitable for\n",
        "a language model, we concatenate inputs with targets putting a separator\n",
        "in between. We also need to create a mask -- with 0s at inputs and 1s at targets -- so that the model is not penalized for mis-predicting the article and only focuses on the summary. See the preprocess function below for how this is done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx24EI-_0Mn5"
      },
      "source": [
        "# Special tokens\n",
        "SEP = 0  # Padding or separator token\n",
        "EOS = 1  # End of sentence token\n",
        "\n",
        "# Concatenate tokenized inputs and targets using 0 as separator.\n",
        "def preprocess(stream):\n",
        "  for (article, summary) in stream:\n",
        "    joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
        "    mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1)   # Accounting for EOS and SEP\n",
        "    yield joint, joint, np.array(mask)\n",
        "\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\n",
        "input_pipeline = trax.data.Serial(\n",
        "    # Tokenizes\n",
        "    trax.data.Tokenize(vocab_dir=\"vocab_dir/\", vocab_file=\"summarize32k.subword.subwords\"),\n",
        "    # Uses function defined above\n",
        "    preprocess,\n",
        "    # Filters out examples longer than 2048\n",
        "    trax.data.FilterByLength(2048)\n",
        ")\n",
        "\n",
        "# Apply preprocessing to data streams.\n",
        "train_stream = input_pipeline(train_stream_fn())\n",
        "eval_stream = input_pipeline(eval_stream_fn())\n",
        "\n",
        "train_input, train_target, train_mask = next(train_stream)\n",
        "\n",
        "# They are the same in Language Model (LM).\n",
        "assert sum((train_input - train_target) ** 2) == 0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-2mHnSVl-R-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11acf39a-7058-4b4d-b106-ba80efb1c05d"
      },
      "source": [
        "# prints mask, 0s on article, 1s on summary\n",
        "print(f\"Single example mask:\\n\\n {train_mask}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Single example mask:\n",
            "\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTPA1tAJngNM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e32e704c-68a1-45bd-947d-1132648051f2"
      },
      "source": [
        "# prints: [Example][<EOS>][<pad>][Example Summary][<EOS>]\n",
        "print(f\"Single example:\\n\\n {detokenize(train_input)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Single example:\n",
            "\n",
            " A curry house has been fined £3,000 after two dead mice were\n",
            "discovered next to a sack of onions. Health inspectors also found\n",
            "rodent droppings in the kitchen and food stored inside a dirty shed\n",
            "when they visited Khans Tandoori and Balti Takeaway in Portsmouth,\n",
            "Hampshire. The kitchen sink was also dry and had no soap and the staff\n",
            "toilet was in a poor condition, while layers of dirt, grease and\n",
            "debris had built up in the areas where food was handled and stored.\n",
            "Khans Tandoori and Balti Takeaway in Portsmouth, Hampshire, has been\n",
            "fined £3,000 after these dead mice were discovered next to a sack of\n",
            "onions . Magistrates ordered the restaurant to pay a £1,800 fine,\n",
            "£1,117 in costs and a £36 victim surcharge, after owner of Harold\n",
            "Southsea Ltd, admitted five charges under hygiene legislation on\n",
            "behalf of the takeaway. Health officer Christopher Larkin from\n",
            "Portsmouth City Council had discovered the dead mice on sticky boards\n",
            "that had been put out to catch the vermin when he visited the\n",
            "restaurant last July. Staff were forced to close the restaurant to\n",
            "make emergency repairs, and it was only allowed to reopen once the\n",
            "kitchen had been cleaned and disinfected, and the food moved out of\n",
            "the lean-to shed. Other improvements were also made. Magistrates\n",
            "ordered the restaurant (pictured) to pay a £1,800 fine, £1,117 in\n",
            "costs and a £36 victim surcharge, after owner of Harold Southsea Ltd,\n",
            "admitted five charges under hygiene legislation on behalf of the\n",
            "takeaway . Inspectors found that the kitchen sink was dry and had no\n",
            "soap, while the staff toilet was in a poor condition . 'Food hygiene\n",
            "is of prime importance and we cannot allow poor standards and\n",
            "behaviour like this,' said Alan Cufley, head of environmental health\n",
            "at the Portsmouth City Council. 'When problems are found, we try to\n",
            "work with businesses to help them improve, but if necessary we will\n",
            "take appropriate action, including prosecution, to protect the\n",
            "public.' The restaurant admitted failing to maintain the premises in\n",
            "good repair, failing to maintain them in a clean condition, failing to\n",
            "protect food from risk of contamination, failing to protect the\n",
            "kitchen from pest entry, and failing to control pests. These bags of\n",
            "cleaning products were discovered when inspectors visited the takeaway\n",
            "in July last year . Mr Khan, who owns the takeaway, said: 'It's been\n",
            "cleaned up. Basically we have done everything that's required. 'We\n",
            "have been trying to improve it for the future. We have never had it\n",
            "this bad before. 'We have taken measures to have a clean kitchen.'\n",
            "Council environment chief Robert New said: 'There's just no excuse for\n",
            "mice to be in kitchens. 'It has been a priority since I took over.\n",
            "It's about protecting the public.' He added there has been a change of\n",
            "strategy at the council, with officers now tackling 'medium to bad'\n",
            "offenders as a priority, rather than looking at each restaurant in\n",
            "turn. Health inspectors also found food being stored inside this dirty\n",
            "shed. The takeaway was forced to close, and could only reopen once the\n",
            "restaurant had been cleaned, and the food removed from the lean-to .\n",
            "Mouse droppings were found on the floor of the kitchen, and in a goods\n",
            "store used by staff .<EOS><pad>Rodentdroppings found in the Khans\n",
            "Tandoori and Balti Takeaway kitchen . Food was being stored in a dirty\n",
            "shed, and the kitchen sink had no soap . Dead mice were discovered on\n",
            "sticky boards intended to catch them . Takeaway fined after owner\n",
            "admitted hygiene charges on its behalf .<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixR9Wn2pnvOc"
      },
      "source": [
        "<a name='1.3'></a>\n",
        "### 1.3 Batching with bucketing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9Ul-LCynxiJ"
      },
      "source": [
        "As in the previous week, we use bucketing to create batches of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bQ9H6YPnprU"
      },
      "source": [
        "# Bucketing to create batched generators.\n",
        "\n",
        "# Buckets are defined in terms of boundaries and batch sizes.\n",
        "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
        "# So below, we'll take a batch of 16 sentences of length < 128 , 8 of length < 256, 4 of length < 512. And so on. \n",
        "boundaries = [128, 256, 512, 1024]\n",
        "batch_sizes = [16, 8, 4, 2, 1]\n",
        "\n",
        "# Create the streams.\n",
        "train_batch_stream = trax.data.BucketByLength(boundaries, batch_sizes)(train_stream)\n",
        "eval_batch_stream = trax.data.BucketByLength(boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cysXURSQovJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "618d131c-0733-42be-d6de-b0af9bfddb4c"
      },
      "source": [
        "# Every execution will result in generation of a different article\n",
        "# Try running this cell multiple times to see how the length of the examples affects the batch size\n",
        "input_batch, _, mask_batch = next(train_batch_stream)\n",
        "\n",
        "# Shape of the input_batch\n",
        "input_batch.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpiR8IrBo_oL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "112ae978-451b-4f52-ad17-dad89db61b55"
      },
      "source": [
        "# print corresponding integer values\n",
        "print(input_batch[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    7  6062 19330    21 15570  3385 10077    23    46 11555   527  1147\n",
            "  1865 23891  6412   379   514  3558  8998  5754   527 17296  6933 18453\n",
            "  2337   320 26377    14   186  6157  2416    23    46 11555   527  1147\n",
            "  1865 23891  6412     3  3385 10077  6503 20889 24810    16    72   441\n",
            "     6   104     6 23324  7511    41   806  1019   134   809    15  9158\n",
            " 15019   410   395   799    28  2145   809  9744  5429   812     3     9\n",
            "  2823     6   104     6   292 13431  1353   592   233    19  8272   527\n",
            "  1147  1865 23891  6412   214   213    72   331    35  6122    87   527\n",
            "    15  4175   117   143    18    46   576   132   213  2297   138  1099\n",
            "  5400    22   379 17895   102   213   347     2   368 10077   127    22\n",
            "  1425   117  3965 19330    21    80   691   213  4967   527   213  2145\n",
            "  1782    13  1006    13    18    46  3965 19330    21     3   326 11498\n",
            "    25 14320 21690   186    13   793   213   864   412   196  1782    13\n",
            "   410    28  3489   157   186    36   527   105   897   156   412    28\n",
            "  7570  5163   114  1682  1480   229  2685   231     3    13  4386    28\n",
            "  3489     2 22057  4955   809   126   186    41 18999 13066    17   539\n",
            "  2002     9  3558  2028   969    22    62    19  4132 26977   320   126\n",
            "   809    15  2300   132   213   531  1782    13    18   133   213   884\n",
            "   107 22825  7633  7824   640   127   285    13     7    75    19   416\n",
            "   320   126  1248   310   116    44  1782    13   169 14149    72  1210\n",
            "     6   104     6   292   331   132   213   797  1779   126  1019   156\n",
            "  2002   863   213  2145   103  1353  6402   368 10077   793 14010  6240\n",
            "   320  4132   117  8191    16    80   506   331   186   285    22  7359\n",
            "  2125   117   506   186  3336  1099 10940  6394 20443     2  1779  8161\n",
            "  3648   320   213   395     2   793 11576   692   368 10077    40 24462\n",
            "   134  4601  4067   223    41 29725     4   165  8191    16     2    41\n",
            " 29725     4   371   211   213  1082     2    33   288  2754    13  1134\n",
            "  3898   171  4195    22  7359  2125   117   506   186  3336  8503   117\n",
            "    69   793   156    22  1353   996  1019   381   186   441     6   104\n",
            "     6 23324     2 21585    20    28  2635  3898   368  6394 20443   127\n",
            "     3   380   527   213   331  6402    22 21302    17    68  1838   936\n",
            "   192   131  1353 14960     4    95   132   979   527    28 16283 12627\n",
            "     2   192   213    54  2663   131  1425   117 26313    20    80  1248\n",
            "   213   117 17066    20     2  4483  1808    80  8998  1782    69  1353\n",
            "  6963    16   156    61   186   246     3    13  1425 26313    14     3\n",
            "    13  1353  1429   320 10924  5849     4   103   236     2   103  1353\n",
            " 12967  8800     3    69    40    15 14114    64   186  1353  5091    16\n",
            "   809   156 11969     7    69   127  4601 27634     4  1918 29725     4\n",
            "    26   545   413   320   126   132    28 15468    26   186 21274   166\n",
            "    13    39    19  1151   475  1478  1019   130  1970   122    51  1435\n",
            "   346  1847  4172 27634    80   368 10077  6503   213 11498     2  7881\n",
            "    16    22    40    28   117  7570  5163   114    80  1018  1248   213\n",
            "    72   331   186   285    77  1353    92  1865 12749     4   936    15\n",
            "  4175  1079   105     3  9305   692   809  9744  5429   812     8   497\n",
            "    12   436   141   668  1170   320   869   213  2823     6   104     6\n",
            "   292 13431   527    38  3748   379 12352   149   527  4814    15  1759\n",
            "    95    36   527   213  2635     7     5  5369    90   285   131    62\n",
            "    19   172  2754  1353    78    15  1588  2423     2    22   127  4477\n",
            " 27439  9275  7583     7    13   358 29725     4    26   288  4872   285\n",
            " 29725     4     5   413  1838  2002  1775    25   441  7511    41   806\n",
            "  1019   368 10077   132   420   186   429     3   207    25  1233   320\n",
            "   213   458  4872    41  2850  1687   571     6    28     6   719   412\n",
            " 16711     4 15564  1283     2    35 15472   102    28   335   711     3\n",
            "   863    31  4332   213   331  2663   368 10077 24386  1172   105    78\n",
            "   647   181    19    41    40  3565 15994 11105  1044   186   117  4001\n",
            "  5352    80   809    36  1091     7     5  2820   132   213   797     3\n",
            "   252    36  5479    36   527   213   331  2663    22 21302    17    68\n",
            "  1838   936   186   117 16144    17    80    15  7122  5928   214    68\n",
            "     3    56  1353   749  6503   691   368 10077   799     3  9305   692\n",
            "   436   668  1170   320   869   134   527    38  1147  3748   990     3\n",
            " 17895   102   213  2145     2   213 13431   127  3611    34 10991  5382\n",
            "    13   934   206   476   539   285   143    18    46   576   213  2297\n",
            "   138  3898  4195    22   117   108    18 21539    17    80    36   527\n",
            "   213   331   132   117    28  7570  5163   114  2324    80   691 20662\n",
            "    16    68  2820     3     9    86    54  1242  1353  7511    22   547\n",
            "    15  1759    78    36   527    31 23885   192   131  1353   477   809\n",
            "    28  1588   320 14012     4  2037   809    68  3368     2    22   127\n",
            "     3   368 10077  6122   320 25166    36   527   213   331    40   117\n",
            "   762    80 22502     5    35   206    19  4437   105     2   186   465\n",
            "    22   108    18   117 15301    80    68  4283  2624  1382   527  3409\n",
            " 10273   103   412  5754     3   305  1353  7456     2    22 13293     2\n",
            "    78   213  1102   527    68 17845     4    35   285    22   640    40\n",
            "   320  1399    68   236  1019  1083   320   126   132  2754    22   742\n",
            "  1353   163 20218  4878   147     3   348   213    55   527   213  6402\n",
            " 24810     5     2   368 10077     7     5   938  1353 10656   527  2518\n",
            "     2   103  1353  1613    10     1     0  3385 10077    23    46   233\n",
            "    19  8272   527  1147  1865 23891  6412 16346 27439  6774  1628     9\n",
            "  2823     6   104     6   292  1353  5754   527   117 26377    16    80\n",
            "   186   117  6157  1532    80 18453  2125 16346 27439  6774  1628  9744\n",
            "  5429   812   793    22  1065 22675   185  1019   117  8191    16   506\n",
            "  2337  6053 27439  6774  1628  9305   692   436   141   668  1170   320\n",
            "   869   213 13431   527    38  3748   824  4979 16346 27439  6774  7583\n",
            "     7  6062 19330    21    80  8998   127 27439  9275  1628  4175 27439\n",
            "  9275  7583     7   108    18    46   576   132  2297   138     7     1\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdpXUf_SpOAJ"
      },
      "source": [
        "Things to notice:\n",
        " - First we see the corresponding values of the words.\n",
        " - The first 1, which represents the `<EOS>` tag of the article.\n",
        " - Followed by a 0, which represents a `<pad>` tag.\n",
        " - After the first 0 (`<pad>` tag) the corresponding values are of the words that are used for the summary of the article.\n",
        " - The second 1 represents the `<EOS>` tag for the summary.\n",
        " - All the trailing 0s represent `<pad>` tags which are appended to maintain consistent length (If you don't see them then it would mean it is already of max length)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti34afulpFfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48dd082e-5a48-4b7b-f715-21b00e875ee9"
      },
      "source": [
        "# print the article and its summary\n",
        "print(\"Article:\\n\\n \", detokenize(input_batch[0]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Article:\n",
            "\n",
            "  'Vindicated': Stephen Perry has been cleared of seven sexual offences\n",
            ". An insurance boss accused of hiring attractive teenage girls to ogle\n",
            "and grope has been cleared of seven sexual offences. Stephen Perry\n",
            "denied sexually assaulting two 17-year-olds when they worked for him\n",
            "at his Rotherham business throughout a trial at Sheffield Crown Court.\n",
            "The 58-year-old widow was today found not guilty of seven sexual\n",
            "offences against the two women but admitted some of his behaviour\n",
            "'could have been taken in the wrong way'. rothe . Speaking after the\n",
            "case, Mr Perry said he felt 'vindicated' by the outcome of the trial.\n",
            "'I feel I have been vindicated. These allegations were unfounded and I\n",
            "told the police as much. 'I am a friendly man and one of them\n",
            "described me as a fatherly figure which is about right. I encouraged a\n",
            "friendly, relaxed atmosphere at work and they misconstrued things.'\n",
            "The insurance chief added he would not hire teenagers to work at his\n",
            "firm in the future. 'I have made the decision like Lionel Blair once\n",
            "said that I'm not going to work with children any more. 'I now employ\n",
            "two 45-year-old women in the office who work for me.' During the trial\n",
            "it was alleged Mr Perry told recruitment managers to hire 'cracking'\n",
            "young women and that he liked employees 'young and fresh'. Sean\n",
            "Kilbride, who supplied candidates to the business, told jurors Mr\n",
            "Perry had instructed him: ''If they’re cracking, they’ll get the job,\n",
            "you know what I mean,' before adding he liked employees 'young and\n",
            "fresh'. 'He told me he was looking for 16 and 17-year-olds, preferably\n",
            "a girl,' Mr Kilbride said. One of the women alleged he grabbed her\n",
            "from behind while she was bent over in front of a filing cabinet,\n",
            "while the other claimed she felt 'uncomfortably' with the 'touchy,\n",
            "feely' boss. 'He was eyeing me up and down. I felt uncomfortable. I\n",
            "was trying to shrug it off, it was disgusting. He had his tongue out\n",
            "and was staring at me . 'He said: \"Don’t ever come to work in a skirt\n",
            "and glasses because I will not be held responsible for my actions if\n",
            "we are left alone.\"' Mr Perry denied the allegations, insisting he had\n",
            "a 'fatherly' relationship with the two women and that there was no\n",
            "sexual motive behind his behaviour towards them. Jurors at Sheffield\n",
            "Crown Court (above) took just 40 minutes to clear the 58-year-old\n",
            "widow of all charges . Accused of putting his hands over one of the\n",
            "girl's faces so that she would not see what was on his computer\n",
            "screen, he said: 'I don’t know where that’s come from.' Both were 17\n",
            "when they worked for Mr Perry in 2012 and 2013. They were sent to the\n",
            "company where they earned £100-a-week as apprentice clerks, but quit\n",
            "after a few months. During their interviews the women claimed Mr Perry\n",
            "quizzed them on whether or not they had boyfriends and 'oohed' at one\n",
            "woman's bottom in the office. On one occasion one of the women claimed\n",
            "he grabbed her from behind and 'rubbed' his crotch against her. This\n",
            "was strong denied by Mr Perry throughout. Jurors took 40 minutes to\n",
            "clear him of all seven charges brought. Speaking after the trial, the\n",
            "widow said: 'In hindsight I probably did say things that could have\n",
            "been taken the wrong way,' adding he 'may have comforted' one of the\n",
            "women in 'a fatherly manner' by patting her bottom. The only other\n",
            "contact was when he put his hands on one of their shoulders while she\n",
            "was working at a computer to poke fun at her height, he said. Mr Perry\n",
            "admitted to observing one of the women had 'big' breasts but did not\n",
            "touch them, and says he may have 'touched' her thigh instead of\n",
            "slapping it as accused. She was hired, he insisted, on the basis of\n",
            "her CV but that he once had to tell her off for coming to work in what\n",
            "he thought was an inappropriate blouse. At the time of the alleged\n",
            "assaults, Mr Perry's wife was dying of cancer, it was\n",
            "heard.<EOS><pad>StephenPerry has been found not guilty of seven sexual\n",
            "offences . The 58-year-old was accused of 'ogling' and 'groping'\n",
            "teenage employees . Sheffield Crown Court told he asked recruiters for\n",
            "'cracking young girls' Jurors took just 40 minutes to clear the widow\n",
            "of all charges this afternoon . 'Vindicated' boss said behaviour 'may\n",
            "have been taken in wrong way'<EOS><pad><pad><pad><pad><pad><pad><pad><\n",
            "pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATpr-_GipgQI"
      },
      "source": [
        "You can see that the data has the following structure:\n",
        "- <span style='color:blue'> [Article] </span> -> `<EOS>` -> `<pad>` -> <span style='color:blue'> [Article Summary] </span> -> `<EOS>` -> (possibly) multiple `<pad>`\n",
        "\n",
        "The loss is taken only on the summary using cross_entropy as loss function. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP7mXo7bpw8z"
      },
      "source": [
        "<a name='2'></a>\n",
        "## Part 2: Summarization with transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4QQrS6Spynl"
      },
      "source": [
        "Now that we have given you the data generator and have handled the preprocessing for you, it is time for you to build your own model. We saved you some time because we know you have already preprocessed data before in this specialization, so we would rather you spend your time doing the next steps. \n",
        "\n",
        "You will be implementing the attention from scratch and then using it in your transformer model. Concretely, you will understand how attention works, how you use it to connect the encoder and the decoder.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deeplearning.ai-NLPS/transformer_decoder_zoomin.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ3o3A86p-2c"
      },
      "source": [
        "<a name='2.1'></a>\n",
        "### 2.1 Dot product attention "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zRcYbLMqDBy"
      },
      "source": [
        "Now you will implement dot product attention which takes in a query, key, value, and a mask. It returns the output. \n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deeplearning.ai-NLPS/dotproduct.png?raw=1' width='800'/>\n",
        "\n",
        "\n",
        "Here are some helper functions that will help you create tensors and display useful information:\n",
        "   - `create_tensor`  creates a `jax numpy array` from a list of lists.\n",
        "   - `display_tensor` prints out the shape and the actual tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLjjOHlHqNud"
      },
      "source": [
        "def create_tensor(t):\n",
        "  \"\"\"Create tensor from list of lists\"\"\"\n",
        "  return jnp.array(t)\n",
        "\n",
        "def display_tensor(t, name):\n",
        "  \"\"\"Display shape and tensor\"\"\"\n",
        "  print(f\"{name} shape: {t.shape}\\n\")\n",
        "  print(f\"{t}\\n\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GThM6N_QGgjF"
      },
      "source": [
        "Before implementing it yourself, you can play around with a toy example of `dot product attention` without the softmax  operation. Technically it would not be `dot product attention` without the softmax but this is done to avoid giving away too much of the answer and the idea is to display these tensors to give you a sense of how they look like.\n",
        "\n",
        "The formula for attention is this one:\n",
        "\n",
        "$$\n",
        "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\\n",
        "$$\n",
        "\n",
        "$d_{k}$ stands for the dimension of queries and keys.\n",
        "\n",
        "The `query`, `key`, `value` and `mask` vectors are provided for this example.\n",
        "\n",
        "Notice that the masking is done using very negative values that will yield a similar effect to using $-\\infty $. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dLTCFWqGeTM",
        "outputId": "1172abb8-101a-4e98-b868-561ce46959ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "q = create_tensor([[1, 0, 0], [0, 1, 0]])\n",
        "display_tensor(q, \"query\")\n",
        "\n",
        "k = create_tensor([[1, 2, 3], [4, 5, 6]])\n",
        "display_tensor(k, \"key\")\n",
        "\n",
        "v = create_tensor([[0, 1, 0], [1, 0, 1]])\n",
        "display_tensor(v, \"value\")\n",
        "\n",
        "m = create_tensor([[0, 0], [-1e9, 0]])\n",
        "display_tensor(m, \"mask\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "query shape: (2, 3)\n",
            "\n",
            "[[1 0 0]\n",
            " [0 1 0]]\n",
            "\n",
            "key shape: (2, 3)\n",
            "\n",
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "\n",
            "value shape: (2, 3)\n",
            "\n",
            "[[0 1 0]\n",
            " [1 0 1]]\n",
            "\n",
            "mask shape: (2, 2)\n",
            "\n",
            "[[ 0.e+00  0.e+00]\n",
            " [-1.e+09  0.e+00]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owdb28CcGkvE"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "query shape: (2, 3)\n",
        "\n",
        "[[1 0 0]\n",
        " [0 1 0]]\n",
        "\n",
        "key shape: (2, 3)\n",
        "\n",
        "[[1 2 3]\n",
        " [4 5 6]]\n",
        "\n",
        "value shape: (2, 3)\n",
        "\n",
        "[[0 1 0]\n",
        " [1 0 1]]\n",
        "\n",
        "mask shape: (2, 2)\n",
        "\n",
        "[[ 0.e+00  0.e+00]\n",
        " [-1.e+09  0.e+00]]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtjGVkdgHwoI",
        "outputId": "0d199eb0-6b37-43d2-aa04-3b6bc7f7638f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "q_dot_k = q @ k.T / jnp.sqrt(3)\n",
        "display_tensor(q_dot_k, \"query dot key\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "query dot key shape: (2, 2)\n",
            "\n",
            "[[0.57735026 2.309401  ]\n",
            " [1.1547005  2.8867512 ]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9wTng9KH8wh"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "query dot key shape: (2, 2)\n",
        "\n",
        "[[0.57735026 2.309401  ]\n",
        " [1.1547005  2.8867514 ]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx-AiKe6IEu4",
        "outputId": "fead05d3-8bc8-48dc-c841-8c07f73ce051",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "masked = q_dot_k + m\n",
        "display_tensor(masked, \"masked query dot key\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "masked query dot key shape: (2, 2)\n",
            "\n",
            "[[ 5.7735026e-01  2.3094010e+00]\n",
            " [-1.0000000e+09  2.8867512e+00]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yY4NpVCIMMe"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "masked query dot key shape: (2, 2)\n",
        "\n",
        "[[ 5.7735026e-01  2.3094010e+00]\n",
        " [-1.0000000e+09  2.8867514e+00]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4uZnEZeIgeY",
        "outputId": "4ca0f182-c90d-4f7a-b3fe-4b80058c21cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "display_tensor(masked @ v, \"masked query dot key dot value\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "masked query dot key dot value shape: (2, 3)\n",
            "\n",
            "[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]\n",
            " [ 2.8867512e+00 -1.0000000e+09  2.8867512e+00]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9yC5J3KIlO9"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "masked query dot key dot value shape: (2, 3)\n",
        "\n",
        "[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]\n",
        " [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyIYO2Y9Inib"
      },
      "source": [
        "In order to use the previous dummy tensors to test some of the graded functions, a batch dimension should be added to them so they mimic the shape of real-life examples. The mask is also replaced by a version of it that resembles the one that is used by trax:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P4sEBieIqLc",
        "outputId": "24da74bd-a811-4f6e-bbe8-1d2ea8a0d23e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "q_with_batch = q[None, :]\n",
        "display_tensor(q_with_batch, \"query with batch dim\")\n",
        "\n",
        "k_with_batch = k[None, :]\n",
        "display_tensor(k_with_batch, \"key with batch dim\")\n",
        "\n",
        "v_with_batch = v[None, :]\n",
        "display_tensor(v_with_batch, \"value with batch dim\")\n",
        "\n",
        "m_bool = create_tensor([[True, True], [False, True]])\n",
        "display_tensor(m_bool, \"boolean mask\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "query with batch dim shape: (1, 2, 3)\n",
            "\n",
            "[[[1 0 0]\n",
            "  [0 1 0]]]\n",
            "\n",
            "key with batch dim shape: (1, 2, 3)\n",
            "\n",
            "[[[1 2 3]\n",
            "  [4 5 6]]]\n",
            "\n",
            "value with batch dim shape: (1, 2, 3)\n",
            "\n",
            "[[[0 1 0]\n",
            "  [1 0 1]]]\n",
            "\n",
            "boolean mask shape: (2, 2)\n",
            "\n",
            "[[ True  True]\n",
            " [False  True]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wh9JjUsJxEW"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "query with batch dim shape: (1, 2, 3)\n",
        "\n",
        "[[[1 0 0]\n",
        "  [0 1 0]]]\n",
        "\n",
        "key with batch dim shape: (1, 2, 3)\n",
        "\n",
        "[[[1 2 3]\n",
        "  [4 5 6]]]\n",
        "\n",
        "value with batch dim shape: (1, 2, 3)\n",
        "\n",
        "[[[0 1 0]\n",
        "  [1 0 1]]]\n",
        "\n",
        "boolean mask shape: (2, 2)\n",
        "\n",
        "[[ True  True]\n",
        " [False  True]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzICG_U2J1A-"
      },
      "source": [
        "<a name='ex01'></a>\n",
        "#### Exercise 01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_JSzfKdKDcJ"
      },
      "source": [
        "**Instructions:** Implement the dot product attention. Concretely, implement the following equation\n",
        "\n",
        "\n",
        "$$\n",
        "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\\n",
        "$$\n",
        "\n",
        "$Q$ - query, \n",
        "$K$ - key, \n",
        "$V$ - values, \n",
        "$M$ - mask, \n",
        "${d_k}$ - depth/dimension of the queries and keys (used for scaling down)\n",
        "\n",
        "You can implement this formula either by `trax` numpy (trax.math.numpy) or regular `numpy` but it is recommended to use `jnp`.\n",
        "\n",
        "Something to take into consideration is that within trax, the masks are tensors of `True/False` values not 0's and $-\\infty$ as in the previous example. Within the graded function don't think of applying the mask by summing up matrices, instead use `jnp.where()` and treat the **mask as a tensor of boolean values with `False` for values that need to be masked and True for the ones that don't.**\n",
        "\n",
        "Also take into account that the real tensors are far more complex than the toy ones you just played with. Because of this avoid using shortened operations such as `@` for dot product or `.T` for transposing. Use `jnp.matmul()` and `jnp.swapaxes()` instead.\n",
        "\n",
        "This is the self-attention block for the transformer decoder. Good luck! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJbLDMDEKES0"
      },
      "source": [
        "# UNQ_C1\n",
        "# GRADED FUNCTION: DotProductAttention\n",
        "def DotProductAttention(query, key, value, mask):\n",
        "  \"\"\"\n",
        "  Dot product self-attention.\n",
        "  Args:\n",
        "      query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)\n",
        "      key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)\n",
        "      value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k\n",
        "      mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)\n",
        "\n",
        "  Returns:\n",
        "      jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
        "  \"\"\"\n",
        "\n",
        "  assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
        "\n",
        "  ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "  # Save depth/dimension of the query embedding for scaling down the dot product\n",
        "  depth = query.shape[-1]\n",
        "\n",
        "  # Calculate scaled query key dot product according to formula above\n",
        "  dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth)\n",
        "\n",
        "  # Apply the mask\n",
        "  if mask is not None:   # The 'None' in this line does not need to be replaced\n",
        "    dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n",
        "\n",
        "  # Softmax formula implementation\n",
        "  # Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers\n",
        "  # Hint: Last axis should be used and keepdims should be True\n",
        "  # Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)\n",
        "  logsumexp = trax.fastmath.logsumexp(dots, axis=-1, keepdims=True)\n",
        "\n",
        "  # Take exponential of dots minus logsumexp to get softmax\n",
        "  # Use jnp.exp()\n",
        "  dots = jnp.exp(dots - logsumexp)\n",
        "\n",
        "  # Multiply dots by value to get self-attention\n",
        "  # Use jnp.matmul()\n",
        "  attention = jnp.matmul(dots, value)\n",
        "\n",
        "  ## END CODE HERE ###\n",
        "\n",
        "  return attention"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFIFoL6NTiqa",
        "outputId": "a0ec63ed-966b-492e-a077-2adf42eb46af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "DotProductAttention(q_with_batch, k_with_batch, v_with_batch, m_bool)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[[0.8496746 , 0.15032546, 0.8496746 ],\n",
              "              [1.        , 0.        , 1.        ]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FIKPI8-U5cU"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "DeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],\n",
        "              [1.        , 0.        , 1.        ]]], dtype=float32)\n",
        "```    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T_NKrUhWZxD"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "\n",
        "### 2.2 Causal Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqCy030WWa5s"
      },
      "source": [
        "Now you are going to implement causal attention: multi-headed attention with a mask to attend only to words that occurred before. \n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deeplearning.ai-NLPS/causal.png?raw=1' width='800'/>\n",
        "\n",
        "In the image above, a word can see everything that is before it, but not what is after it. To implement causal attention, you will have to transform vectors and do many reshapes. You will need to implement the functions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtMwUGjiWqWT"
      },
      "source": [
        "<a name='ex02'></a>\n",
        "#### Exercise 02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My6akHp5Wr3R"
      },
      "source": [
        "Implement the following functions that will be needed for Causal Attention:\n",
        "\n",
        "- <span style='color:blue'>**compute_attention_heads**</span>: Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\\times$ n_heads, seqlen, d_head).\n",
        "- <span style='color:blue'>**dot_product_self_attention**</span>: Creates a mask matrix with `False` values above the diagonal and `True` values below and calls DotProductAttention which implements dot product self attention.\n",
        "- <span style='color:blue'>**compute_attention_output**</span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads $\\times$ d_head). These operations concatenate (stack/merge) the heads. \n",
        "\n",
        "Next there are some toy tensors which may serve to give you an idea of the data shapes and opperations involved in Causal Attention. They are also useful to test out your functions! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whGbRAZaTpRS",
        "outputId": "506dcf7d-0dac-492d-eb3c-78e687dfe4ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tensor2d = create_tensor(q)\n",
        "display_tensor(tensor2d, \"query matrix (2D tensor)\")\n",
        "\n",
        "tensor4d2b = create_tensor([[q, q], [q, q]])\n",
        "display_tensor(tensor4d2b, \"batch of two (multi-head) collections of query matrices (4D tensor)\")\n",
        "\n",
        "tensor3dc = create_tensor([jnp.concatenate([q, q], axis=-1)])\n",
        "display_tensor(tensor3dc, \"one batch of concatenated heads of query matrices (3d tensor)\")\n",
        "\n",
        "tensor3dc3b = create_tensor([jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1)])\n",
        "display_tensor(tensor3dc3b, \"three batches of concatenated heads of query matrices (3d tensor)\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "query matrix (2D tensor) shape: (2, 3)\n",
            "\n",
            "[[1 0 0]\n",
            " [0 1 0]]\n",
            "\n",
            "batch of two (multi-head) collections of query matrices (4D tensor) shape: (2, 2, 2, 3)\n",
            "\n",
            "[[[[1 0 0]\n",
            "   [0 1 0]]\n",
            "\n",
            "  [[1 0 0]\n",
            "   [0 1 0]]]\n",
            "\n",
            "\n",
            " [[[1 0 0]\n",
            "   [0 1 0]]\n",
            "\n",
            "  [[1 0 0]\n",
            "   [0 1 0]]]]\n",
            "\n",
            "one batch of concatenated heads of query matrices (3d tensor) shape: (1, 2, 6)\n",
            "\n",
            "[[[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]]\n",
            "\n",
            "three batches of concatenated heads of query matrices (3d tensor) shape: (3, 2, 6)\n",
            "\n",
            "[[[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFAjGcFYaU0H"
      },
      "source": [
        "It is important to know that the following 3 functions would normally be defined within the `CausalAttention` function further below. \n",
        "\n",
        "However this makes these functions harder to test. Because of this, these functions are shown individually using a `closure` (when necessary) that simulates them being inside of the `CausalAttention` function. This is done because they rely on some variables that can be accessed from within `CausalAttention`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNfI1iAYanF3"
      },
      "source": [
        "#### Support Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4BP09Vaao-f"
      },
      "source": [
        "<span style='color:blue'>**compute_attention_heads**</span>: Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\\times$ n_heads, seqlen, d_head).\n",
        "\n",
        "**For the closures you only have to fill the inner function.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZjO94ZtawKT"
      },
      "source": [
        "# UNQ_C2\n",
        "# GRADED FUNCTION: compute_attention_heads_closure\n",
        "def compute_attention_heads_closure(n_heads, d_head):\n",
        "  \"\"\"\n",
        "  Function that simulates environment inside CausalAttention function.\n",
        "  Args:\n",
        "      d_head (int):  dimensionality of heads.\n",
        "      n_heads (int): number of attention heads.\n",
        "  Returns:\n",
        "      function: compute_attention_heads function\n",
        "  \"\"\"\n",
        "\n",
        "  def compute_attention_heads(x):\n",
        "    \"\"\"\n",
        "    Compute the attention heads.\n",
        "    Args:\n",
        "        x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
        "    Returns:\n",
        "        jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
        "    \"\"\"\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "    # Size of the x's batch dimension\n",
        "    batch_size = x.shape[0]\n",
        "    # Length of the sequence\n",
        "    # Should be size of x's first dimension without counting the batch dim\n",
        "    seqlen = x.shape[1]\n",
        "\n",
        "    # Reshape x using jnp.reshape()\n",
        "    # batch_size, seqlen, n_heads*d_head -> batch_size, seqlen, n_heads, d_head\n",
        "    x = jnp.reshape(x, (batch_size, seqlen, n_heads, d_head))\n",
        "\n",
        "    # Transpose x using jnp.transpose()\n",
        "    # batch_size, seqlen, n_heads, d_head -> batch_size, n_heads, seqlen, d_head\n",
        "    # Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them\n",
        "    x = jnp.transpose(x, (0, 2, 1, 3))\n",
        "\n",
        "    # Reshape x using jnp.reshape()\n",
        "    # batch_size, n_heads, seqlen, d_head -> batch_size*n_heads, seqlen, d_head\n",
        "    x = jnp.reshape(x, (-1, seqlen, d_head))\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return x\n",
        "\n",
        "  return compute_attention_heads"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wiamdms8VNo7",
        "outputId": "a43d5cb0-eb9a-400a-c57e-9db151215dfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "display_tensor(tensor3dc3b, \"input tensor\")\n",
        "result_cah = compute_attention_heads_closure(2, 3)(tensor3dc3b)\n",
        "display_tensor(result_cah, \"output tensor\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input tensor shape: (3, 2, 6)\n",
            "\n",
            "[[[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]]\n",
            "\n",
            "output tensor shape: (6, 2, 3)\n",
            "\n",
            "[[[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phop_O0vV5IU"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "input tensor shape: (3, 2, 6)\n",
        "\n",
        "[[[1 0 0 1 0 0]\n",
        "  [0 1 0 0 1 0]]\n",
        "\n",
        " [[1 0 0 1 0 0]\n",
        "  [0 1 0 0 1 0]]\n",
        "\n",
        " [[1 0 0 1 0 0]\n",
        "  [0 1 0 0 1 0]]]\n",
        "\n",
        "output tensor shape: (6, 2, 3)\n",
        "\n",
        "[[[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]]\n",
        "```\n",
        "\n",
        "<span style='color:blue'> **dot_product_self_attention** </span>: Creates a mask matrix with `False` values above the diagonal and `True` values below and calls DotProductAttention which implements dot product self attention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKq6H7HFVmnw"
      },
      "source": [
        "# UNQ_C3\n",
        "# GRADED FUNCTION: dot_product_self_attention\n",
        "def dot_product_self_attention(q, k, v):\n",
        "  \"\"\"\n",
        "  Masked dot product self attention.\n",
        "  Args:\n",
        "      q (jax.interpreters.xla.DeviceArray): queries.\n",
        "      k (jax.interpreters.xla.DeviceArray): keys.\n",
        "      v (jax.interpreters.xla.DeviceArray): values.\n",
        "  Returns:\n",
        "      jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.\n",
        "  \"\"\"\n",
        "\n",
        "  ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "  # Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)\n",
        "  # NOTE: there is a revision underway with the autograder to tolerate better indexing. \n",
        "  # Until then, please index q.shape using negative values (this is equivalent to counting from right to left)\n",
        "  mask_size = q.shape[-2]\n",
        "\n",
        "  # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
        "  # Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_\n",
        "  # Use jnp.tril() - Lower triangle of an array and jnp.ones()\n",
        "  mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
        "\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return DotProductAttention(q, k, v, mask)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpVOhvh6FVlT",
        "outputId": "7b5a7f65-30c2-4fc5-c1a8-6601b333cbf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dot_product_self_attention(q_with_batch, k_with_batch, v_with_batch)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[[0.        , 1.        , 0.        ],\n",
              "              [0.8496746 , 0.15032548, 0.8496746 ]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3tMQ2DIFbi8"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "DeviceArray([[[0.        , 1.        , 0.        ],\n",
        "              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)\n",
        "```\n",
        "\n",
        "<span style='color:blue'> **compute_attention_output** </span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads $\\times$ d_head). These operations concatenate (stack/merge) the heads. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cs7fM2bFi9I"
      },
      "source": [
        "# UNQ_C4\n",
        "# GRADED FUNCTION: compute_attention_output_closure\n",
        "def compute_attention_output_closure(n_heads, d_head):\n",
        "  \"\"\"\n",
        "  Function that simulates environment inside CausalAttention function.\n",
        "  Args:\n",
        "      d_head (int):  dimensionality of heads.\n",
        "      n_heads (int): number of attention heads.\n",
        "  Returns:\n",
        "      function: compute_attention_output function\n",
        "  \"\"\"\n",
        "\n",
        "  def compute_attention_output(x):\n",
        "    \"\"\"\n",
        "    Compute the attention output.\n",
        "    Args:\n",
        "        x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
        "    Returns:\n",
        "        jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
        "    \"\"\"\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "        \n",
        "    # Length of the sequence\n",
        "    # Should be size of x's first dimension without counting the batch dim\n",
        "    seqlen = x.shape[1]\n",
        "\n",
        "    # Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)\n",
        "    x = jnp.reshape(x, (-1, n_heads, seqlen, d_head))\n",
        "\n",
        "    # Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)\n",
        "    x = jnp.transpose(x, (0, 2, 1, 3))\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Reshape to allow to concatenate the heads\n",
        "\n",
        "    return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
        "\n",
        "  return compute_attention_output"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-pXvtdwHMEc",
        "outputId": "0074147a-2925-4439-82a7-6fffab838bd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "display_tensor(result_cah, \"input tensor\")\n",
        "result_cao = compute_attention_output_closure(2,3)(result_cah)\n",
        "display_tensor(result_cao, \"output tensor\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input tensor shape: (6, 2, 3)\n",
            "\n",
            "[[[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]]\n",
            "\n",
            "output tensor shape: (3, 2, 6)\n",
            "\n",
            "[[[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LppqTunOHQtK"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "input tensor shape: (6, 2, 3)\n",
        "\n",
        "[[[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]]\n",
        "\n",
        "output tensor shape: (3, 2, 6)\n",
        "\n",
        "[[[1 0 0 1 0 0]\n",
        "  [0 1 0 0 1 0]]\n",
        "\n",
        " [[1 0 0 1 0 0]\n",
        "  [0 1 0 0 1 0]]\n",
        "\n",
        " [[1 0 0 1 0 0]\n",
        "  [0 1 0 0 1 0]]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r25tZntrHRNU"
      },
      "source": [
        "#### Causal Attention Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqvtOGjIHTOi"
      },
      "source": [
        "Now it is time for you to put everything together within the `CausalAttention` or Masked multi-head attention function:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deeplearning.ai-NLPS/masked-attention.png?raw=1' width='800'/>\n",
        "\n",
        "**Instructions:** Implement the causal attention.\n",
        "Your model returns the causal attention through a $tl.Serial$ with the following:\n",
        "\n",
        "- <span style='color:blue'> [tl.Branch](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch) </span>: consisting of 3 [tl.Dense(d_feature), ComputeAttentionHeads] to account for the queries, keys, and values.\n",
        "- <span style='color:blue'> [tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn)</span>: Takes in dot_product_self_attention function and uses it to compute the dot product using $Q$, $K$, $V$.\n",
        "- <span style='color:blue'> [tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn)</span>: Takes in restack_attention_heads to allow for parallel computing.\n",
        "- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense)</span>: Final Dense layer, with dimension `d_feature`.\n",
        "\n",
        "Remember that in order for trax to properly handle the functions you just defined, they need to be added as layers using the [`tl.Fn()`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn) function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY7zKhqVHxl-"
      },
      "source": [
        "# UNQ_C5\n",
        "# GRADED FUNCTION: CausalAttention\n",
        "def CausalAttention(d_feature,\n",
        "                    n_heads,\n",
        "                    compute_attention_heads_closure=compute_attention_heads_closure,\n",
        "                    dot_product_self_attention=dot_product_self_attention,\n",
        "                    compute_attention_output_closure=compute_attention_output_closure,\n",
        "                    mode=\"train\"):\n",
        "  \"\"\"\n",
        "  Transformer-style multi-headed causal attention.\n",
        "\n",
        "  Args:\n",
        "      d_feature (int):  dimensionality of feature embedding.\n",
        "      n_heads (int): number of attention heads.\n",
        "      compute_attention_heads_closure (function): Closure around compute_attention heads.\n",
        "      dot_product_self_attention (function): dot_product_self_attention function. \n",
        "      compute_attention_output_closure (function): Closure around compute_attention_output. \n",
        "      mode (str): 'train' or 'eval'.\n",
        "\n",
        "  Returns:\n",
        "      trax.layers.combinators.Serial: Multi-headed self-attention model.\n",
        "  \"\"\"\n",
        "\n",
        "  assert d_feature % n_heads == 0\n",
        "  d_head = d_feature // n_heads\n",
        "\n",
        "  ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "  # HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)\n",
        "  # Since you are dealing with closures you might need to call the outer \n",
        "  # function with the correct parameters to get the actual uncalled function.\n",
        "  ComputeAttentionHeads = tl.Fn(\"AttnHeads\", compute_attention_heads_closure(n_heads, d_head), n_out=1)\n",
        "\n",
        "  return tl.Serial(\n",
        "      tl.Branch(  # creates three towers for one input, takes activations and creates queries keys and values\n",
        "          [tl.Dense(d_feature), ComputeAttentionHeads], # queries\n",
        "          [tl.Dense(d_feature), ComputeAttentionHeads], # keys\n",
        "          [tl.Dense(d_feature), ComputeAttentionHeads], # values\n",
        "      ),\n",
        "      tl.Fn(\"DotProductAttn\", dot_product_self_attention, n_out=1),  # takes QKV\n",
        "      # HINT: The second argument to tl.Fn() is an uncalled function\n",
        "      # Since you are dealing with closures you might need to call the outer \n",
        "      # function with the correct parameters to get the actual uncalled function.\n",
        "      tl.Fn(\"AttnOutput\", compute_attention_output_closure(n_heads, d_head), n_out=1), # to allow for parallel\n",
        "      tl.Dense(d_feature)   # Final dense layer\n",
        "  )\n",
        "\n",
        "  ### END CODE HERE ###"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSY6TM018xED",
        "outputId": "88734b5e-f3bc-4aef-9e05-f32495149f4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Take a look at the causal attention model\n",
        "print(CausalAttention(d_feature=512, n_heads=8))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Branch_out3[\n",
            "    [Dense_512, AttnHeads]\n",
            "    [Dense_512, AttnHeads]\n",
            "    [Dense_512, AttnHeads]\n",
            "  ]\n",
            "  DotProductAttn_in3\n",
            "  AttnOutput\n",
            "  Dense_512\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l-MQmxE88oM"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "Serial[\n",
        "  Branch_out3[\n",
        "    [Dense_512, AttnHeads]\n",
        "    [Dense_512, AttnHeads]\n",
        "    [Dense_512, AttnHeads]\n",
        "  ]\n",
        "  DotProductAttn_in3\n",
        "  AttnOutput\n",
        "  Dense_512\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlKh_Vxh89LP"
      },
      "source": [
        "<a name='2.3'></a>\n",
        "### 2.3 Transformer decoder block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LMZgTfh9Aar"
      },
      "source": [
        "Now that you have implemented the causal part of the transformer, you will implement the transformer decoder block. Concretely you will be implementing this image now.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deeplearning.ai-NLPS/transformer_decoder_1.png?raw=1' width='200'/>\n",
        "\n",
        "To implement this function, you will have to call the `CausalAttention` or Masked multi-head attention function you implemented above. You will have to add a feedforward which consists of: \n",
        "\n",
        "- <span style='color:blue'> [tl.LayerNorm](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm) </span>: used to layer normalize\n",
        "- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: the dense layer\n",
        "- <span style='color:blue'> [ff_activation](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu) </span>: feed forward activation (we use ReLu) here.\n",
        "- <span style='color:blue'> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout) </span>: dropout layer\n",
        "- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: dense layer\n",
        "- <span style='color:blue'> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout) </span>: dropout layer\n",
        "\n",
        "Finally once you implement the feedforward, you can go ahead and implement the entire block using: \n",
        "\n",
        "- <span style='color:blue'> [tl.Residual](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual) </span>: takes in the tl.LayerNorm(), causal attention block, tl.dropout. \n",
        "\n",
        "- <span style='color:blue'> [tl.Residual](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual) </span>: takes in the feedforward block you will implement. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDRNFgUg9zEN"
      },
      "source": [
        "<a name='ex03'></a>\n",
        "#### Exercise 03\n",
        "**Instructions:** Implement the transformer decoder block. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgwEcwsC9QSJ"
      },
      "source": [
        "# UNQ_C6\n",
        "# GRADED FUNCTION: DecoderBlock\n",
        "def DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation):\n",
        "  \"\"\"\n",
        "  Returns a list of layers that implements a Transformer decoder block.\n",
        "\n",
        "  The input is an activation tensor.\n",
        "\n",
        "  Args:\n",
        "      d_model (int):  depth of embedding.\n",
        "      d_ff (int): depth of feed-forward layer.\n",
        "      n_heads (int): number of attention heads.\n",
        "      dropout (float): dropout rate (how much to drop out).\n",
        "      mode (str): 'train' or 'eval'.\n",
        "      ff_activation (function): the non-linearity in feed-forward layer.\n",
        "\n",
        "  Returns:\n",
        "      list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
        "  \"\"\"\n",
        "\n",
        "  ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "  # Create masked multi-head attention block using CausalAttention function\n",
        "  causal_attention = CausalAttention(d_model, n_heads=n_heads, mode=mode)\n",
        "\n",
        "  # Create feed-forward block (list) with two dense layers with dropout and input normalized\n",
        "  feed_forward = [\n",
        "      # Normalize layer inputs\n",
        "      tl.LayerNorm(),\n",
        "      # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n",
        "      tl.Dense(d_ff),\n",
        "      # Add activation function passed in as a parameter (you need to call it!)\n",
        "      ff_activation(),  # Generally ReLU\n",
        "      # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
        "      tl.Dropout(rate=dropout, mode=mode),\n",
        "      # Add second feed forward layer (don't forget to set the correct value for n_units)\n",
        "      tl.Dense(d_model),\n",
        "      # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
        "      tl.Dropout(rate=dropout, mode=mode)            \n",
        "  ]\n",
        "\n",
        "  # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n",
        "  return [\n",
        "     tl.Residual(\n",
        "         # Normalize layer input\n",
        "         tl.LayerNorm(),\n",
        "         # Add causal attention block previously defined (without parentheses)\n",
        "         causal_attention,\n",
        "         # Add dropout with rate and mode specified\n",
        "         tl.Dropout(rate=dropout, mode=mode)\n",
        "     ),\n",
        "     tl.Residual(\n",
        "         # Add feed forward block (without parentheses)\n",
        "         feed_forward\n",
        "     ),    \n",
        "  ]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t88ObmMK-FnD",
        "outputId": "b0f5c016-5eb2-495e-9b3f-e9d24d44c0f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Take a look at the decoder block\n",
        "print(DecoderBlock(d_model=512, d_ff=2048, n_heads=8, dropout=0.1, mode='train', ff_activation=tl.Relu))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Serial[\n",
            "  Branch_out2[\n",
            "    None\n",
            "    Serial[\n",
            "      LayerNorm\n",
            "      Serial[\n",
            "        Branch_out3[\n",
            "          [Dense_512, AttnHeads]\n",
            "          [Dense_512, AttnHeads]\n",
            "          [Dense_512, AttnHeads]\n",
            "        ]\n",
            "        DotProductAttn_in3\n",
            "        AttnOutput\n",
            "        Dense_512\n",
            "      ]\n",
            "      Dropout\n",
            "    ]\n",
            "  ]\n",
            "  Add_in2\n",
            "], Serial[\n",
            "  Branch_out2[\n",
            "    None\n",
            "    Serial[\n",
            "      LayerNorm\n",
            "      Dense_2048\n",
            "      Serial[\n",
            "        Relu\n",
            "      ]\n",
            "      Dropout\n",
            "      Dense_512\n",
            "      Dropout\n",
            "    ]\n",
            "  ]\n",
            "  Add_in2\n",
            "]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRLPvXGlCUmW"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "[Serial[\n",
        "  Branch_out2[\n",
        "    None\n",
        "    Serial[\n",
        "      LayerNorm\n",
        "      Serial[\n",
        "        Branch_out3[\n",
        "          [Dense_512, AttnHeads]\n",
        "          [Dense_512, AttnHeads]\n",
        "          [Dense_512, AttnHeads]\n",
        "        ]\n",
        "        DotProductAttn_in3\n",
        "        AttnOutput\n",
        "        Dense_512\n",
        "      ]\n",
        "      Dropout\n",
        "    ]\n",
        "  ]\n",
        "  Add_in2\n",
        "], Serial[\n",
        "  Branch_out2[\n",
        "    None\n",
        "    Serial[\n",
        "      LayerNorm\n",
        "      Dense_2048\n",
        "      Relu\n",
        "      Dropout\n",
        "      Dense_512\n",
        "      Dropout\n",
        "    ]\n",
        "  ]\n",
        "  Add_in2\n",
        "]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbOTc67sCZv-"
      },
      "source": [
        "<a name='2.4'></a>\n",
        "### 2.4 Transformer Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JVw1WGiCdUT"
      },
      "source": [
        "You will now bring it all together. In this part you will use all the subcomponents you previously built to make the final model. Concretely, here is the image you will be implementing. \n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deeplearning.ai-NLPS/transformer_decoder.png?raw=1' width='300'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulu8z3jtCpg1"
      },
      "source": [
        "<a name='ex04'></a>\n",
        "#### Exercise 04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOXd_4O5CqxH"
      },
      "source": [
        "**Instructions:** Previously you coded the decoder block. Now you will code the transformer language model. Here is what you will need. \n",
        "\n",
        "- <span style=\"color:blue\"> positional_enconder </span>- a list containing the following layers:\n",
        "    - <span style=\"color:blue\"> [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding)\n",
        "    - <span style=\"color:blue\"> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout)\n",
        "    - <span style=\"color:blue\"> [tl.PositionalEncoding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PositionalEncoding)\n",
        "\n",
        "- A list of `n_layers` <span style=\"color:blue\"> decoder blocks</span>.\n",
        "- <span style=\"color:blue\"> [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial): </span> takes in the following layers or lists of layers:\n",
        "    - <span style=\"color:blue\"> [tl.ShiftRight](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight): </span>: shift the tensor to the right by padding on axis 1.\n",
        "    - <span style=\"color:blue\"> positional_encoder </span>: encodes the text positions.\n",
        "    - <span style=\"color:blue\"> decoder_blocks </span>: the ones you created.\n",
        "    - <span style=\"color:blue\"> [tl.LayerNorm](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm) </span>: a layer norm.\n",
        "    - <span style=\"color:blue\"> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: takes in the vocab_size.\n",
        "    - <span style=\"color:blue\"> [tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax) </span>: to predict.\n",
        "    \n",
        "Go go go!! You can do it :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLKVaJ--Cw1i"
      },
      "source": [
        "# UNQ_C7\n",
        "# GRADED FUNCTION: TransformerLM\n",
        "def TransformerLM(vocab_size=33300, d_model=512, d_ff=2048, n_layers=6, n_heads=8, dropout=0.1, max_len=4096, mode=\"train\", ff_activation=tl.Relu):\n",
        "  \"\"\"\n",
        "  Returns a Transformer language model.\n",
        "\n",
        "  The input to the model is a tensor of tokens. (This model uses only the decoder part of the overall Transformer.)\n",
        "\n",
        "  Args:\n",
        "      vocab_size (int): vocab size.\n",
        "      d_model (int):  depth of embedding.\n",
        "      d_ff (int): depth of feed-forward layer.\n",
        "      n_layers (int): number of decoder layers.\n",
        "      n_heads (int): number of attention heads.\n",
        "      dropout (float): dropout rate (how much to drop out).\n",
        "      max_len (int): maximum symbol length for positional encoding.\n",
        "      mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n",
        "      ff_activation (function): the non-linearity in feed-forward layer.\n",
        "\n",
        "  Returns:\n",
        "      trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens to activations over a vocab set.\n",
        "  \"\"\"\n",
        "\n",
        "  ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "  # Embedding inputs and positional encoder\n",
        "  positional_encoder = [\n",
        "     # Add embedding layer of dimension (vocab_size, d_model)\n",
        "     tl.Embedding(vocab_size, d_model),\n",
        "     # Use dropout with rate and mode specified\n",
        "     tl.Dropout(rate=dropout, mode=mode),\n",
        "     # Add positional encoding layer with maximum input length and mode specified\n",
        "     tl.PositionalEncoding(max_len=max_len, mode=mode)                   \n",
        "  ]\n",
        "\n",
        "  # Create stack (list) of decoder blocks with n_layers with necessary parameters\n",
        "  decoder_blocks = [\n",
        "     DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)               \n",
        "  ]\n",
        "\n",
        "  # Create the complete model as written in the figure\n",
        "  return tl.Serial(\n",
        "      # Use teacher forcing (feed output of previous step to current step)\n",
        "      tl.ShiftRight(mode=mode),   # Specify the mode!\n",
        "      # Add positional encoder\n",
        "      positional_encoder,\n",
        "      # Add decoder blocks\n",
        "      decoder_blocks,\n",
        "      # Normalize layer\n",
        "      tl.LayerNorm(),\n",
        "\n",
        "      # Add dense layer of vocab_size (since need to select a word to translate to)\n",
        "      # (a.k.a., logits layer. Note: activation already set by ff_activation)\n",
        "      tl.Dense(vocab_size),\n",
        "      # Get probabilities with Logsoftmax\n",
        "      tl.LogSoftmax(),\n",
        "  )\n",
        "\n",
        "  ### END CODE HERE ###"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7S5arCFHMoc",
        "outputId": "53b68627-1060-4f4b-925f-36a4350cac55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Take a look at the Transformer\n",
        "print(TransformerLM(n_layers=1))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_33300_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Branch_out3[\n",
            "            [Dense_512, AttnHeads]\n",
            "            [Dense_512, AttnHeads]\n",
            "            [Dense_512, AttnHeads]\n",
            "          ]\n",
            "          DotProductAttn_in3\n",
            "          AttnOutput\n",
            "          Dense_512\n",
            "        ]\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_33300\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jOM6eV4HfNF"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "Serial[\n",
        "  ShiftRight(1)\n",
        "  Embedding_33300_512\n",
        "  Dropout\n",
        "  PositionalEncoding\n",
        "  Serial[\n",
        "    Branch_out2[\n",
        "      None\n",
        "      Serial[\n",
        "        LayerNorm\n",
        "        Serial[\n",
        "          Branch_out3[\n",
        "            [Dense_512, AttnHeads]\n",
        "            [Dense_512, AttnHeads]\n",
        "            [Dense_512, AttnHeads]\n",
        "          ]\n",
        "          DotProductAttn_in3\n",
        "          AttnOutput\n",
        "          Dense_512\n",
        "        ]\n",
        "        Dropout\n",
        "      ]\n",
        "    ]\n",
        "    Add_in2\n",
        "  ]\n",
        "  Serial[\n",
        "    Branch_out2[\n",
        "      None\n",
        "      Serial[\n",
        "        LayerNorm\n",
        "        Dense_2048\n",
        "        Relu\n",
        "        Dropout\n",
        "        Dense_512\n",
        "        Dropout\n",
        "      ]\n",
        "    ]\n",
        "    Add_in2\n",
        "  ]\n",
        "  LayerNorm\n",
        "  Dense_33300\n",
        "  LogSoftmax\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ0EpWR0HiJS"
      },
      "source": [
        "<a name='3'></a>\n",
        "## Part 3: Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKuDKDlnHkzo"
      },
      "source": [
        "Now you are going to train your model. As usual, you have to define the cost function, the optimizer, and decide whether you will be training it on a `gpu` or `cpu`. In this case, you will train your model on a cpu for a few steps and we will load in a pre-trained model that you can use to predict with your own words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neBNEbdPHnop"
      },
      "source": [
        "<a name='3.1'></a>\n",
        "### 3.1 Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZcVD4ZqHqID"
      },
      "source": [
        "You will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set. Each iteration is defined as an `epoch`. For each epoch, you have to go over all the data, using your training iterator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nCkjPULHsUq"
      },
      "source": [
        "<a name='ex05'></a>\n",
        "#### Exercise 05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dmZ7tp6HvyH"
      },
      "source": [
        "**Instructions:** Implement the `train_model` program below to train the neural network above. Here is a list of things you should do:\n",
        "\n",
        "- Create the train task by calling [`trax.supervised.training.TrainTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) and pass in the following: \n",
        "    - <span style='color:blue'> labeled_data </span> = train_gen\n",
        "    - <span style='color:blue'> loss_fn </span> = [tl.CrossEntropyLoss()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss)\n",
        "    - <span style='color:blue'> optimizer </span> = [trax.optimizers.Adam(0.01)](https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam)\n",
        "    - <span style='color:blue'> lr_schedule </span> = [lr_schedule](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.lr_schedules.warmup_and_rsqrt_decay)\n",
        "\n",
        "\n",
        "- Create the eval task by calling [`trax.supervised.training.EvalTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) and pass in the following: \n",
        "    - <span style='color:blue'> labeled_data </span> = eval_gen\n",
        "    - <span style='color:blue'> metrics </span> = tl.CrossEntropyLoss() and [tl.Accuracy()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy)\n",
        "    \n",
        "    \n",
        "- Create the training loop by calling [`trax.supervised.Training.Loop`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) and pass in the following: \n",
        "    - <span style='color:blue'> TransformerLM </span> \n",
        "    - <span style='color:blue'> train_task </span> \n",
        "    - <span style='color:blue'> eval_task </span> = [eval_task]\n",
        "    - <span style='color:blue'> output_dir</span> = output_dir\n",
        "    \n",
        "You will be using a cross entropy loss, with Adam optimizer. Please read the [Trax](https://trax-ml.readthedocs.io/en/latest/index.html) documentation to get a full understanding. \n",
        "\n",
        "The training loop that this function returns can be runned using the `run()` method by passing in the desired number of steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL1wnRTAHzNz"
      },
      "source": [
        "from trax.supervised import training\n",
        "\n",
        "# UNQ_C8\n",
        "# GRADED FUNCTION: train_model\n",
        "def training_loop(TransformerLM, train_gen, eval_gen, output_dir=\"~/model\"):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "      TransformerLM (trax.layers.combinators.Serial): The model you are building.\n",
        "      train_gen (generator): Training stream of data.\n",
        "      eval_gen (generator): Evaluation stream of data.\n",
        "      output_dir (str): folder to save your file.\n",
        "      \n",
        "  Returns:\n",
        "      trax.supervised.training.Loop: Training loop.\n",
        "  \"\"\"\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}