{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment-2--transformer-summarizer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMT6vC+oBN+1BHRb1hf3gNk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/coursera-natural-language-processing-specialization/blob/master/course-4-natural-language-processing-with-attention-models/week-2-Text%2520Summarization/assignment_2_transformer_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELYyBdfz_z9r"
      },
      "source": [
        "## Assignment 2: Transformer Summarizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we-CD4y-ArpW"
      },
      "source": [
        "Welcome to the second assignment of course 4. In this assignment you will explore summarization using the transformer model. Yes, you will implement the transformer decoder from scratch, but we will slowly walk you through it. There are many hints in this notebook so feel free to use them as needed. \n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deeplearning.ai-NLPS/transformerNews.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUOFZXD8Lasw"
      },
      "source": [
        "## Outline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46eAV3qnLb1-"
      },
      "source": [
        "- [Introduction](#0)\n",
        "- [Part 1: Importing the dataset](#1)\n",
        "    - [1.1 Encode & Decode helper functions](#1.1)\n",
        "    - [1.2 Defining parameters](#1.2)\n",
        "    - [1.3 Exploring the data](#1.3)\n",
        "- [Part 2: Summarization with transformer](#2)\n",
        "    - [2.1 Dot product attention](#2.1)\n",
        "        - [Exercise 01](#ex01)\n",
        "    - [2.2 Causal Attention](#2.2)\n",
        "        - [Exercise 02](#ex02)\n",
        "    - [2.3 Transformer decoder block](#2.3)\n",
        "        - [Exercise 03](#ex03)\n",
        "    - [2.4 Transformer Language model](#2.4)\n",
        "        - [Exercise 04](#ex04)\n",
        "- [Part 3: Training](#3)\n",
        "    - [3.1 Training the model](#3.1)\n",
        "        - [Exercise 05](#ex05)\n",
        "- [Part 4: Evaluation](#4)\n",
        "    - [4.1 Loading in a trained model](#4.1)\n",
        "- [Part 5: Testing with your own input](#5) \n",
        "    - [Exercise 6](#ex06)\n",
        "    - [5.1 Greedy decoding](#5.1)\n",
        "        - [Exercise 07](#ex07)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YXMf-JWLemg"
      },
      "source": [
        "<a name='0'></a>\n",
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYuym2inLjHA"
      },
      "source": [
        "Summarization is an important task in natural language processing and could be useful for a consumer enterprise. For example, bots can be used to scrape articles, summarize them, and then you can use sentiment analysis to identify the sentiment about certain stocks. Anyways who wants to read an article or a long email today, when you can build a transformer to summarize text for you. \n",
        "\n",
        "Let's get started, by completing this assignment you will learn to:  \n",
        "\n",
        "- Use built-in functions to preprocess your data\n",
        "- Implement Dot-Product Attention\n",
        "- Implement Causal Attention\n",
        "- Understand how attention works\n",
        "- Build the transformer model\n",
        "- Evaluate your model\n",
        "- Summarize an article\n",
        "\n",
        "As you can tell, this model is slightly different than the ones you have already implemented. This is heavily based on attention and does not rely on sequences, which allows for parallel computing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDCnUwxALncn"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D8zArgGwST9"
      },
      "source": [
        "!pip -q install trax==1.3.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4fLNMc5LsJ4"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)\n",
        "\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as jnp\n",
        "\n",
        "# to print the entire np array\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROzYQv_rERgR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "034ec095-5132-4a59-e157-f6b2607a417f"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# wget -q https://github.com/rahiakela/coursera-natural-language-processing-specialization/raw/master/course-4-natural-language-processing-with-attention-models/week-2-Text%20Summarization/data/cnn_dailymail.zip\n",
        "wget -q https://github.com/rahiakela/coursera-natural-language-processing-specialization/raw/master/course-4-natural-language-processing-with-attention-models/week-2-Text%20Summarization/data/vocab_dir/summarize32k.subword.subwords\n",
        "\n",
        "mkdir vocab_dir\n",
        "cp summarize32k.subword.subwords vocab_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ioVVYpTLo7H"
      },
      "source": [
        "<a name='1'></a>\n",
        "## Part 1: Importing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL0y-KzvLyZK"
      },
      "source": [
        "Trax makes it easy to work with Tensorflow's datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5ILG8RcL0nM"
      },
      "source": [
        "# This will download the dataset if no data_dir is specified.\n",
        "# Downloading and processing can take bit of time, so we have the data already in 'data/' for you\n",
        "#!mkdir data\n",
        "\n",
        "# Importing CNN/DailyMail articles dataset\n",
        "train_stream_fn = trax.data.TFDS(\"cnn_dailymail\", data_dir=\"data/\", keys=(\"article\", \"highlights\"), train=True)\n",
        "\n",
        "# This should be much faster as the data is downloaded already.\n",
        "eval_stream_fn = trax.data.TFDS(\"cnn_dailymail\", data_dir=\"data/\", keys=(\"article\", \"highlights\"), train=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6QwT_N0zxCv"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "### 1.1 Tokenize & Detokenize helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7QV5qORzyke"
      },
      "source": [
        "Just like in the previous assignment, the cell above loads in the encoder for you. Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your [Trax](https://github.com/google/trax) models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: \n",
        "\n",
        "- <span style='color:blue'> word2Ind: </span> a dictionary mapping the word to its index.\n",
        "- <span style='color:blue'> ind2Word:</span> a dictionary mapping the index to its word.\n",
        "- <span style='color:blue'> word2Count:</span> a dictionary mapping the word to the number of times it appears. \n",
        "- <span style='color:blue'> num_words:</span> total number of words that have appeared. \n",
        "\n",
        "Since you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:\n",
        "\n",
        "- <span style='color:blue'> tokenize: </span> converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords.\n",
        "- <span style='color:blue'> detokenize: </span> converts a token list to its corresponding sentence (i.e. string)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DCJo2sLzwCs"
      },
      "source": [
        "def tokenize(input_str, EOS=1):\n",
        "  \"\"\"Input str to features dict, ready for inference\"\"\"\n",
        "\n",
        "  # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
        "  # we get around it by making a 1-element stream with `iter`.\n",
        "  inputs = next(trax.data.tokenize(iter([input_str]), vocab_dir=\"vocab_dir/\", vocab_file=\"summarize32k.subword.subwords\"))\n",
        "\n",
        "  # Mark the end of the sentence with EOS\n",
        "  return list(inputs) + [EOS]\n",
        "\n",
        "def detokenize(integers):\n",
        "  \"\"\"List of ints to str\"\"\"\n",
        "\n",
        "  s = trax.data.detokenize(integers, vocab_dir=\"vocab_dir/\", vocab_file=\"summarize32k.subword.subwords\")\n",
        "\n",
        "  return wrapper.fill(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCdFkvE30IBp"
      },
      "source": [
        "<a name='1.2'></a>\n",
        "### 1.2 Preprocessing for Language Models: Concatenate It!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSBdhxLt0J_H"
      },
      "source": [
        "This week you will use a language model -- Transformer Decoder -- to solve\n",
        "an input-output problem. As you know, language models only predict the next\n",
        "word, they have no notion of inputs. To create a single input suitable for\n",
        "a language model, we concatenate inputs with targets putting a separator\n",
        "in between. We also need to create a mask -- with 0s at inputs and 1s at targets -- so that the model is not penalized for mis-predicting the article and only focuses on the summary. See the preprocess function below for how this is done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx24EI-_0Mn5"
      },
      "source": [
        "# Special tokens\n",
        "SEP = 0  # Padding or separator token\n",
        "EOS = 1  # End of sentence token\n",
        "\n",
        "# Concatenate tokenized inputs and targets using 0 as separator.\n",
        "def preprocess(stream):\n",
        "  for (article, summary) in stream:\n",
        "    joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
        "    mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1)   # Accounting for EOS and SEP\n",
        "    yield joint, joint, np.array(mask)\n",
        "\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\n",
        "input_pipeline = trax.data.Serial(\n",
        "    # Tokenizes\n",
        "    trax.data.Tokenize(vocab_dir=\"vocab_dir/\", vocab_file=\"summarize32k.subword.subwords\"),\n",
        "    # Uses function defined above\n",
        "    preprocess,\n",
        "    # Filters out examples longer than 2048\n",
        "    trax.data.FilterByLength(2048)\n",
        ")\n",
        "\n",
        "# Apply preprocessing to data streams.\n",
        "train_stream = input_pipeline(train_stream_fn())\n",
        "eval_stream = input_pipeline(eval_stream_fn())\n",
        "\n",
        "train_input, train_target, train_mask = next(train_stream)\n",
        "\n",
        "# They are the same in Language Model (LM).\n",
        "assert sum((train_input - train_target) ** 2) == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-2mHnSVl-R-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "381163f3-9504-4c2d-f7c0-088923d2463b"
      },
      "source": [
        "# prints mask, 0s on article, 1s on summary\n",
        "print(f\"Single example mask:\\n\\n {train_mask}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Single example mask:\n",
            "\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTPA1tAJngNM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d3a9828-8063-4cb8-9caf-ee07d5f14aba"
      },
      "source": [
        "# prints: [Example][<EOS>][<pad>][Example Summary][<EOS>]\n",
        "print(f\"Single example:\\n\\n {detokenize(train_input)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Single example:\n",
            "\n",
            " President Barack Obama is getting off the island. In a rare move for\n",
            "him, the president planned a break in the middle of his Martha's\n",
            "Vineyard vacation to return to Washington on Sunday night for meetings\n",
            "with Vice President Joe Biden and other advisers on the U.S. military\n",
            "campaign in Iraq and tensions between police and protesters in\n",
            "Ferguson, Missouri. The White House has been cagey about why the\n",
            "president needs to be back in Washington for those discussions. He's\n",
            "received multiple briefings on both issues while on vacation. The\n",
            "White House had also already announced Obama's plans to return to\n",
            "Washington before the U.S. airstrikes in Iraq began and before the\n",
            "shooting of a teen in Ferguson that sparked protests. Protective\n",
            "gesture: Obama walks with daughter Malia Obama to board Air Force One\n",
            "at Cape Cod Coast Guard Air Station in Massachusetts on Sunday . Back\n",
            "home: Obama and Malia are seen at Joint Base Andrews in Washington\n",
            "early Monday . Mysterious: The White House has been cagey about why\n",
            "the president needs to be back in Washington. He is seen here on the\n",
            "South Lawn of the White House with daughter Malia . In good spirits:\n",
            "Despite the early return, The President and First Daughter seemed to\n",
            "be enjoying a joke . Part of the decision to head back to Washington\n",
            "appears aimed at countering criticism that Obama is spending two weeks\n",
            "on a resort island in the midst of so many foreign and domestic\n",
            "crises. Yet those crises turned the first week of Obama's vacation\n",
            "into a working holiday. He made on-camera statements Iraq and the\n",
            "clashes in Ferguson, a St. Louis suburb. He also called foreign\n",
            "leaders to discuss the tensions between Ukraine and Russia, as well as\n",
            "between Israel and Hamas. 'I think it's fair to say there are, of\n",
            "course, ongoing complicated situations in the world, and that's why\n",
            "you've seen the president stay engaged,' White House spokesman Eric\n",
            "Schultz said. Obama returned from his break along with his 16-year-old\n",
            "daughter Mailia, but is scheduled to return to Martha's Vineyard on\n",
            "Tuesday and stay through next weekend. In a first for Obama family\n",
            "summer vacations, neither teenager is spending the entire holiday with\n",
            "her father. Obama left Washington Aug. 9 with his wife, Michelle,\n",
            "daughter Malia, and the family's two Portuguese water dogs. The White\n",
            "House said 13-year-old Sasha would join her parents at a later date\n",
            "for \"part of their stay\" on this quaint island of shingled homes. But\n",
            "Malia will not be around when her younger sister arrives. The\n",
            "daughters essentially are trading places, and the vacation is boiling\n",
            "down to Obama getting about a week with each one. Malia returned to\n",
            "Washington with her father  and is not expected to go back to Martha's\n",
            "Vineyard. The White House said Sasha will join her parents this week,\n",
            "without saying when she will arrive or what kept her away last week,\n",
            "or why Malia left the island. President Barack Obama bike rides with\n",
            "daughter Malia Obama while on vacation with his family on the island\n",
            "of Martha's Vineyard . Obama often draws chuckles from sympathetic\n",
            "parents who understand his complaints about his girls' lack of\n",
            "interest in spending time with him. 'What I'm discovering is that each\n",
            "year, I get more excited about spending time with them. They get a\n",
            "little less excited,' Obama told CNN last year. Even though work has\n",
            "occupied much of Obama's first week on vacation, he still found plenty\n",
            "of time to golf, go to the beach with his family and go out to dinner\n",
            "on the island. He hit the golf course one more time Sunday ahead of\n",
            "his departure, joining two aides and former NBA player Alonzo Mourning\n",
            "for an afternoon round. He then joined wife Michelle for an evening\n",
            "jazz performance featuring singer Rachelle Ferrell. Obama's vacation\n",
            "has also been infused with a dose of politics. He headlined a\n",
            "fundraiser on the island for Democratic Senate candidates and attended\n",
            "a birthday party for Democratic adviser Vernon Jordan's wife, where he\n",
            "spent time with former President Bill Clinton and Hillary Rodham\n",
            "Clinton. That get-together between the former rivals-turned-partners\n",
            "added another complicated dynamic to Obama's vacation. Just as Obama\n",
            "was arriving on Martha's Vineyard, an interview with the former\n",
            "secretary of state was published in which she levied some of her\n",
            "sharpest criticism of Obama's foreign policy. Clinton later promised\n",
            "she and Obama would 'hug it out' when they saw each other at Jordan's\n",
            "party. No reporters were allowed in, so it's not clear whether there\n",
            "was any hugging, but the White House said the president danced to\n",
            "nearly every song.<EOS><pad>PresidentObama will head back to the White\n",
            "House on Sunday night as tensions rise in Missouri and Iraq . The\n",
            "decision appears aimed at countering criticism that the president was\n",
            "spending two weeks on a resort island in the midst of so many crises\n",
            ".<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixR9Wn2pnvOc"
      },
      "source": [
        "<a name='1.3'></a>\n",
        "### 1.3 Batching with bucketing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9Ul-LCynxiJ"
      },
      "source": [
        "As in the previous week, we use bucketing to create batches of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bQ9H6YPnprU"
      },
      "source": [
        "# Bucketing to create batched generators.\n",
        "\n",
        "# Buckets are defined in terms of boundaries and batch sizes.\n",
        "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
        "# So below, we'll take a batch of 16 sentences of length < 128 , 8 of length < 256, 4 of length < 512. And so on. \n",
        "boundaries = [128, 256, 512, 1024]\n",
        "batch_sizes = [16, 8, 4, 2, 1]\n",
        "\n",
        "# Create the streams.\n",
        "train_batch_stream = trax.data.BucketByLength(boundaries, batch_sizes)(train_stream)\n",
        "eval_batch_stream = trax.data.BucketByLength(boundaries, batch_sizes)(eval_stream)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cysXURSQovJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a39ea7a-77c9-4c82-cbab-336d36f57408"
      },
      "source": [
        "# Every execution will result in generation of a different article\n",
        "# Try running this cell multiple times to see how the length of the examples affects the batch size\n",
        "input_batch, _, mask_batch = next(train_batch_stream)\n",
        "\n",
        "# Shape of the input_batch\n",
        "input_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1175)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpiR8IrBo_oL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3540de-9bd8-4f9a-e16f-14e9533a5f89"
      },
      "source": [
        "# print corresponding integer values\n",
        "print(input_batch[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1668  4375 12915 10077  1595   592    15 18621   320 21427     4    61\n",
            "   320    32    88   226  1668   276 10583  3345   320    15   205     7\n",
            "     5  1698  2572     2  1480   229    43   213   184    10    59     3\n",
            "  2572  1248  2058  1782   198   379    49  1151    92   404  1098   341\n",
            "  2572  1098     2   186 22015  1998    18  1537   403   379   208    28\n",
            "  1375  1019   213  1080   266 29725     4     5  2565   320  4182    89\n",
            "  2572  3898   213  3837  4375   127   809    28   782  1900   824  4979\n",
            "  1782     9   715    13   410 20688   592    39 11832   824  2854   570\n",
            "     6    78   691   379 12830  1049    89  1338   320  5421   213 26246\n",
            "    84   889     2   455  3223 22728  5889   186   379   727 19064  1779\n",
            " 13862     4   213  1646   527   101   467  1668   186   379   561  2002\n",
            "  1324   320    28 15324     4 10256    17   532   220   736   320     9\n",
            " 25316     4     2   213  2462   715    39   919  1668 22752     5   281\n",
            "   225   446    28   984     3  9175  6051     4   246  1019   846   379\n",
            " 24408     4  4706   239    11  1668  4375 12915 10077   127   824  4979\n",
            "   285    22   229 21427    16    61   320    32    88   226  1668   276\n",
            " 10583  3345   320   213   205     7     5  1698  2572     2  1480   229\n",
            "    43   213   184    10    59     3  2572  1248  2058   379 23736  9799\n",
            "    20     2 10077    23 24462   213  1668   676   527  1347  8048   320\n",
            "   835  3282   132   213  7117 24026   786  1912   201   809    28  5782\n",
            "   919   527   281    32    10    53   446     3 10493    17   500     2\n",
            "   213    72  1589    39   919   281    65   446    28   719     2   213\n",
            " 15324     4 10209   834     2   186   117   103   229    19   869  4872\n",
            "   213   750    39   413   379  1838   132   213  2705    80    54    74\n",
            "   117   442  1385    80   370   107   557   662   181  3061     3     9\n",
            "  1624   132  2572  1660  1589  2243    28 18304   527  1187   199   310\n",
            " 10127    71   213   184    10    59     3  1838  2058     3  1356    74\n",
            "  2577    88   226 14160   310     2   125   527  1457  1435 19357 20981\n",
            " 17859    21     2    18 14513   918  2251   213   296   254   220   104\n",
            "     2   186   213   266  4150   285  1549  1464    88   226    39  7720\n",
            "   691   213   704   527   824   104     3   184    10    59     3 24684\n",
            " 26156     4    23    46  2881  9347   691   213  4680  5849    14     2\n",
            "   186   213  1080   266   229  1746  1260    64   527   750   320   662\n",
            "  1019   213   310     3  1608   229   132   213   428   527 18994    28\n",
            "   281    53    10   112  2117  4010  1767  2418  1838   699 26651  4217\n",
            "   285    62  1422   977   750   320   213  2863   810     2    35   638\n",
            "  9229  1469 13341    67   527   213  1155     7     5   840     3  5717\n",
            " 24299    20   605   527   213   750  4217     7     5  4823  1019    62\n",
            "   273  1629  1474 16325  2511   320   213   310   192  1709   445    62\n",
            "   273  1629  4925   213   105   320    31   278   628     3  4142     2\n",
            "  9229    62   107   320   172   737   320    28   472 15259   378   285\n",
            "  2193   213   266   320   616   310  1838   442     6 12703  9451   431\n",
            "   628  1779   344    61   809   213  2572   557 11837   627   186   509\n",
            "   428   171    41    49  1151  1233   278     3     9  7166   428   376\n",
            "  1094   711     2   186   188    91     2  5308 24921    61  3681   186\n",
            " 13365    16   246   213  9815 13277   361   428     3     9  1155    40\n",
            "  3640  2935   320   377    28  8010  1036   527   213   472  2407   132\n",
            "    15  2418   320  1608   285    62    18  1325   213   676   527  7074\n",
            "   268  3283   320  3251   213   117 14715    80   320 23036     4   213\n",
            "   617   428   691  1657   310   213  3466   320 15571 20837  6876   955\n",
            "   278     3  4217  8703   246   809   213   220  2425   102  3666  2050\n",
            "  6559  1838  3645 26792  5889     3 10077   475    28   782  1900  1248\n",
            "  8480   747 12099 15553    26     2   231     2   824  4979   132  8134\n",
            "     2  1668     2   320  9168 11579   213 20135     3 10077   127   213\n",
            "   276 10583  3345    25  1048   320  5421 19064   285  1435 17889    16\n",
            "    28 18304   527   310   186  1144  5709   213   184    10    59     3\n",
            " 14513   918   379  1312    19   569   132  4217     7     5  2418   320\n",
            "  1608  1353  1767  1019    28   276 10583 20135   320   213  2572    70\n",
            "   579   638  7049   240 26698   186   213  1668  4375    40   148   316\n",
            "    78   213  1155   320   124     3  9229   476    28   276 10583  1367\n",
            "   229  1048   809   370   527   208  1881   320   399 24684 26156     4\n",
            "  2846 18246   246    78 20588   185   186  1935 26246   902     3    34\n",
            "    28   882   320   882   920  1248  4217  7511   213  1155   408   320\n",
            "  1668    72  1492  1008 10077   457  1065   213  1155   320 21427     4\n",
            "   213   276 10583   123    28 21053 10027   114  4851 16718    35  4217\n",
            " 15194    17     3 10077   229   169   892  2413    71    15   221  1759\n",
            "     2  8676    15   221   284   527  3345   246   320   213  7117 24026\n",
            "  1912   320  2511   378  4606  1631     3   303 10630  8962   117  5773\n",
            "    20    80  9958 13589 12650     2    28 17376  1779  2667  2572   513\n",
            " 17128 21749     4     2 14651   213  3837  4375     7     5 20135   412\n",
            " 16399  1133  6053 27884     4     9 26246   902 27872   391  1435   892\n",
            "  2900   527   213  1359  3898    22   793   213 25316     4  1782   200\n",
            "    89   280   378  4606  1838   213 26319     4 29725     4     5  3849\n",
            "   527   213   224  5614   320   213   224   864  7151  1435   892   662\n",
            "   527   213  1359  1782    56   229    28  1424  1106     2    19    28\n",
            "   726  1106     3   397    51   317   229    44  1010   320  4132    44\n",
            " 25510   283     2  4132    44 24684 26156     4  3898  9958 13589 12650\n",
            "   127  1782   326  1435   506   101     2   141  1144  1083   467     3\n",
            "   207     7   165    19  4574     3   207     7   165    19  4123  4144\n",
            "  2002     9 10256    17 15324     4    78   213   276 10583 20135  2845\n",
            " 13804    45   285   103   229    28   117 22531  4505  1700   527   213\n",
            "  2572  3898   487     3   244 10077     7     5   797  6107 17946    21\n",
            "   592   285  3345    62   117   126 10943  9569   186   384   691   384\n",
            "  1248   378  4606  1631  2002  9958 13589 12650    43  5754 10077     2\n",
            "  1779  1086 24792   213  2572  1248 10940 24976   554   412   160   527\n",
            "    28   707  1019  4958  1652     2   527   144 22871 11705   165   132\n",
            "    15  2575  2685   213  1359   809   213  2572  1782   337   379    97\n",
            "  7230  1083   246   320  2572     2    41   358     7    26   662  2685\n",
            " 13542   379   213   917     2    41   141   483   320   191    28   351\n",
            "   340  3898    22   127    10     1     0  1668  4375 12915 10077  1595\n",
            "   824  4979   285    22    39 14037     4    61   320    32    88   226\n",
            "  1668   276 10583  3345   320   213  2572 16346 27439  6774  1628     9\n",
            " 20135    39   919  1668 22752     5   281   225   446    28   984     2\n",
            "   931   320    28 10256    17 15324     4 16346 27439  6774  1628 10077\n",
            "    23  1065  4217  1547   450   320  2733   213   276 10583   320   213\n",
            "  2572    35  4217  8283 25370    16 16346 27439  6774  1628    27  1668\n",
            " 26792  2009   127   213  3837  5365   229    19 19677   114  2331  2685\n",
            "   213  2572     2    22   141  2976   320   385  1716  2104     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdpXUf_SpOAJ"
      },
      "source": [
        "Things to notice:\n",
        " - First we see the corresponding values of the words.\n",
        " - The first 1, which represents the `<EOS>` tag of the article.\n",
        " - Followed by a 0, which represents a `<pad>` tag.\n",
        " - After the first 0 (`<pad>` tag) the corresponding values are of the words that are used for the summary of the article.\n",
        " - The second 1 represents the `<EOS>` tag for the summary.\n",
        " - All the trailing 0s represent `<pad>` tags which are appended to maintain consistent length (If you don't see them then it would mean it is already of max length)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti34afulpFfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a4d177-42ed-4141-83e9-0197ba093b4e"
      },
      "source": [
        "# print the article and its summary\n",
        "print(\"Article:\\n\\n \", detokenize(input_batch[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Article:\n",
            "\n",
            "  Texas Governor Rick Perry announced today his intentions to deploy up\n",
            "to 1,000 Texas National Guard troops to his state's southern border,\n",
            "which is also the U.S. border with Mexico. 'There . can be no national\n",
            "security without border security, and Texans have paid too . high a\n",
            "price for the federal government’s failure to secure our border,' the\n",
            "Republican Governor said at a news conference this afternoon. 'The\n",
            "action I am ordering today will tackle this crisis head-on by .\n",
            "multiplying our efforts to combat the cartel activity, human\n",
            "traffickers and . individual criminals who threaten the safety of\n",
            "people across Texas and . America.' According to a memo leaked late\n",
            "last night to The Monitor, the executive action will cost Texas\n",
            "taxpayers $12 million a month. Scroll down for video . Done waiting\n",
            "around: Texas Governor Rick Perry said this afternoon that he is\n",
            "deploying up to 1,000 Texas National Guard troops to the state's\n",
            "southern border, which is also the U.S. border with Mexico . Already,\n",
            "Perry has instructed the Texas Department of Public Safety to increase\n",
            "personnel in the Rio Grande River Valley area at a weekly cost of $1.3\n",
            "million. Added together, the two measures will cost $5 million a week,\n",
            "the memo reportedly states, and 'it is not clear where the money will\n",
            "come . from in the budget' other than 'non critical' areas like health\n",
            "care or transportation. The rise in border protection measures follows\n",
            "a surge of Central American children streaming into the U.S. from\n",
            "Mexico. More than 57,000 immigrant children, many of whom are\n",
            "unaccompanied, have illegally entered the country since last year, and\n",
            "the government estimates that approximately 90,000 will arrive by the\n",
            "close of this year. U.S. Border Patrol has been overloaded by the\n",
            "deluge, and the federal government is quickly running out of money to\n",
            "care for the children. Congress is in the process of reviewing a $3.7\n",
            "billion emergency funding request from President Barack Obama that\n",
            "would appropriate additional money to the agencies involved, but House\n",
            "Republicans remain skeptical of the president's plan. Roughly half of\n",
            "the money Obama's asking for would go toward providing humanitarian\n",
            "aid to the children while relatively little would go toward returning\n",
            "the them to their home countries. Furthermore, Republicans would like\n",
            "to see changes to a 2008 trafficking law that requires the government\n",
            "to give children from non-contiguous countries who show up at the\n",
            "border health screenings and due process before they can be sent home.\n",
            "The judicial process often takes months, and even years, clogging up\n",
            "courts and slowing down the repatriation process. The president had\n",
            "initially planned to include a revised version of the 2008 legislation\n",
            "in his request to Congress that would have allowed the Department of\n",
            "Homeland Security to exercise the 'discretion' to bypass the current\n",
            "process by giving children the option to voluntarily return home.\n",
            "Obama backed down at the last minute after receiving negative feedback\n",
            "from Democratic lawmakers. Perry held a news conference with Attorney\n",
            "General Greg Abbott, right, this afternoon in Austin, Texas, to\n",
            "formally announce the deployment. Perry said the National Guard troops\n",
            "were needed to combat criminals that are exploiting a surge of\n",
            "children and families entering the U.S. illegally . Also not included\n",
            "in Obama's request to Congress was funding for a National Guard\n",
            "deployment to the border - something House Speaker John Boehner and\n",
            "the Texas Governor had both called on the president to do. Republicans\n",
            "say a National Guard presence is needed at areas of high crime to help\n",
            "Border Patrol agents crack down on smugglers and drug cartels. In a\n",
            "face to face meeting with Obama when the president came to Texas two\n",
            "weeks ago Perry again asked the president to deploy the National Guard\n",
            "through a federally funded statue but Obama resisted. Perry is now\n",
            "taking matters into his own hands, sending his own set of troops down\n",
            "to the Rio Grande Valley to aid law enforcement officials. State\n",
            "Senator Juan 'Chuy' Hinojosa, a Democrat who represents border town\n",
            "McAllen, criticized the Republican Governor's deployment as\n",
            "unnecessary. '[The cartels] are taking advantage of the situation,' he\n",
            "told the Monitor. 'But our local law enforcement from the sheriff’s\n",
            "offices of the different counties to the different police departments\n",
            "are taking care of the situation. 'This is a civil matter, not a\n",
            "military matter. What we need is more resources to hire more deputies,\n",
            "hire more Border Patrol,' Hinojosa said. 'These are young people, just\n",
            "families coming across. They're not armed. They're not carrying\n",
            "weapons.' The leaked memo on the National Guard deployment\n",
            "specifically denies that it is a 'militarization of the border,'\n",
            "however. And Perry's office reiterated today that troops would 'work\n",
            "seamlessly and side by side with law enforcement officials.' Hinojosa\n",
            "also accused Perry, who recently toured the border with Sean Hannity\n",
            "as part of a special for Fox News, of being insincere in his concern\n",
            "about the situation at the border. 'All . these politicians coming\n",
            "down to border, they don't care about solving . the problem, they just\n",
            "want to make a political point,' he said.<EOS><pad>TexasGovernor Rick\n",
            "Perry announced this afternoon that he will dispatch up to 1,000 Texas\n",
            "National Guard troops to the border . The deployment will cost Texas\n",
            "taxpayers $12 million a month, according to a leaked memo . Perry has\n",
            "asked Obama multiple times to send the National Guard to the border\n",
            "but Obama keeps refusing . A Texas lawmaker said the Republican\n",
            "governor is not sincerely concerned about the border, he just wants to\n",
            "play politics .<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATpr-_GipgQI"
      },
      "source": [
        "You can see that the data has the following structure:\n",
        "- <span style='color:blue'> [Article] </span> -> `<EOS>` -> `<pad>` -> <span style='color:blue'> [Article Summary] </span> -> `<EOS>` -> (possibly) multiple `<pad>`\n",
        "\n",
        "The loss is taken only on the summary using cross_entropy as loss function. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP7mXo7bpw8z"
      },
      "source": [
        "<a name='2'></a>\n",
        "## Part 2: Summarization with transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4QQrS6Spynl"
      },
      "source": [
        "Now that we have given you the data generator and have handled the preprocessing for you, it is time for you to build your own model. We saved you some time because we know you have already preprocessed data before in this specialization, so we would rather you spend your time doing the next steps. \n",
        "\n",
        "You will be implementing the attention from scratch and then using it in your transformer model. Concretely, you will understand how attention works, how you use it to connect the encoder and the decoder.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deeplearning.ai-NLPS/transformer_decoder_zoomin.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ3o3A86p-2c"
      },
      "source": [
        "<a name='2.1'></a>\n",
        "### 2.1 Dot product attention "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zRcYbLMqDBy"
      },
      "source": [
        "Now you will implement dot product attention which takes in a query, key, value, and a mask. It returns the output. \n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deeplearning.ai-NLPS/dotproduct.png?raw=1' width='800'/>\n",
        "\n",
        "\n",
        "Here are some helper functions that will help you create tensors and display useful information:\n",
        "   - `create_tensor`  creates a `jax numpy array` from a list of lists.\n",
        "   - `display_tensor` prints out the shape and the actual tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLjjOHlHqNud"
      },
      "source": [
        "def create_tensor(t):\n",
        "  \"\"\"Create tensor from list of lists\"\"\"\n",
        "  return jnp.array(t)\n",
        "\n",
        "def display_tensor(t, name):\n",
        "  \"\"\"Display shape and tensor\"\"\"\n",
        "  print(f\"{name} shape: {t.shape}\\n\")\n",
        "  print(f\"{t}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GThM6N_QGgjF"
      },
      "source": [
        "Before implementing it yourself, you can play around with a toy example of `dot product attention` without the softmax  operation. Technically it would not be `dot product attention` without the softmax but this is done to avoid giving away too much of the answer and the idea is to display these tensors to give you a sense of how they look like.\n",
        "\n",
        "The formula for attention is this one:\n",
        "\n",
        "$$\n",
        "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\\n",
        "$$\n",
        "\n",
        "$d_{k}$ stands for the dimension of queries and keys.\n",
        "\n",
        "The `query`, `key`, `value` and `mask` vectors are provided for this example.\n",
        "\n",
        "Notice that the masking is done using very negative values that will yield a similar effect to using $-\\infty $. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dLTCFWqGeTM"
      },
      "source": [
        "q = create_tensor([[1, 0, 0], [0, 1, 0]])\n",
        "display_tensor(q, \"query\")\n",
        "\n",
        "k = create_tensor([[1, 2, 3], [4, 5, 6]])\n",
        "display_tensor(k, \"key\")\n",
        "\n",
        "v = create_tensor([[0, 1, 0], [1, 0, 1]])\n",
        "display_tensor(v, \"value\")\n",
        "\n",
        "m = create_tensor([[0, 0], [-1e9, 0]])\n",
        "display_tensor(m, \"mask\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owdb28CcGkvE"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "query shape: (2, 3)\n",
        "\n",
        "[[1 0 0]\n",
        " [0 1 0]]\n",
        "\n",
        "key shape: (2, 3)\n",
        "\n",
        "[[1 2 3]\n",
        " [4 5 6]]\n",
        "\n",
        "value shape: (2, 3)\n",
        "\n",
        "[[0 1 0]\n",
        " [1 0 1]]\n",
        "\n",
        "mask shape: (2, 2)\n",
        "\n",
        "[[ 0.e+00  0.e+00]\n",
        " [-1.e+09  0.e+00]]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtjGVkdgHwoI"
      },
      "source": [
        "q_dot_k = q @ k.T / jnp.sqrt(3)\n",
        "display_tensor(q_dot_k, \"query dot key\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9wTng9KH8wh"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "query dot key shape: (2, 2)\n",
        "\n",
        "[[0.57735026 2.309401  ]\n",
        " [1.1547005  2.8867514 ]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx-AiKe6IEu4"
      },
      "source": [
        "masked = q_dot_k + m\n",
        "display_tensor(masked, \"masked query dot key\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yY4NpVCIMMe"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "masked query dot key shape: (2, 2)\n",
        "\n",
        "[[ 5.7735026e-01  2.3094010e+00]\n",
        " [-1.0000000e+09  2.8867514e+00]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4uZnEZeIgeY"
      },
      "source": [
        "display_tensor(masked @ v, \"masked query dot key dot value\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9yC5J3KIlO9"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "masked query dot key dot value shape: (2, 3)\n",
        "\n",
        "[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]\n",
        " [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyIYO2Y9Inib"
      },
      "source": [
        "In order to use the previous dummy tensors to test some of the graded functions, a batch dimension should be added to them so they mimic the shape of real-life examples. The mask is also replaced by a version of it that resembles the one that is used by trax:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P4sEBieIqLc"
      },
      "source": [
        "q_with_batch = q[None, :]\n",
        "display_tensor(q_with_batch, \"query with batch dim\")\n",
        "\n",
        "k_with_batch = k[None, :]\n",
        "display_tensor(k_with_batch, \"key with batch dim\")\n",
        "\n",
        "v_with_batch = v[None, :]\n",
        "display_tensor(v_with_batch, \"value with batch dim\")\n",
        "\n",
        "m_bool = create_tensor([[True, True], [False, True]])\n",
        "display_tensor(m_bool, \"boolean mask\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wh9JjUsJxEW"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "query with batch dim shape: (1, 2, 3)\n",
        "\n",
        "[[[1 0 0]\n",
        "  [0 1 0]]]\n",
        "\n",
        "key with batch dim shape: (1, 2, 3)\n",
        "\n",
        "[[[1 2 3]\n",
        "  [4 5 6]]]\n",
        "\n",
        "value with batch dim shape: (1, 2, 3)\n",
        "\n",
        "[[[0 1 0]\n",
        "  [1 0 1]]]\n",
        "\n",
        "boolean mask shape: (2, 2)\n",
        "\n",
        "[[ True  True]\n",
        " [False  True]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzICG_U2J1A-"
      },
      "source": [
        "<a name='ex01'></a>\n",
        "#### Exercise 01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_JSzfKdKDcJ"
      },
      "source": [
        "**Instructions:** Implement the dot product attention. Concretely, implement the following equation\n",
        "\n",
        "\n",
        "$$\n",
        "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\\n",
        "$$\n",
        "\n",
        "$Q$ - query, \n",
        "$K$ - key, \n",
        "$V$ - values, \n",
        "$M$ - mask, \n",
        "${d_k}$ - depth/dimension of the queries and keys (used for scaling down)\n",
        "\n",
        "You can implement this formula either by `trax` numpy (trax.math.numpy) or regular `numpy` but it is recommended to use `jnp`.\n",
        "\n",
        "Something to take into consideration is that within trax, the masks are tensors of `True/False` values not 0's and $-\\infty$ as in the previous example. Within the graded function don't think of applying the mask by summing up matrices, instead use `jnp.where()` and treat the **mask as a tensor of boolean values with `False` for values that need to be masked and True for the ones that don't.**\n",
        "\n",
        "Also take into account that the real tensors are far more complex than the toy ones you just played with. Because of this avoid using shortened operations such as `@` for dot product or `.T` for transposing. Use `jnp.matmul()` and `jnp.swapaxes()` instead.\n",
        "\n",
        "This is the self-attention block for the transformer decoder. Good luck! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJbLDMDEKES0"
      },
      "source": [
        "# UNQ_C1\n",
        "# GRADED FUNCTION: DotProductAttention\n",
        "def DotProductAttention(query, key, value, mask):\n",
        "  \"\"\"\n",
        "  Dot product self-attention.\n",
        "  Args:\n",
        "      query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)\n",
        "      key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)\n",
        "      value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k\n",
        "      mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)\n",
        "\n",
        "  Returns:\n",
        "      jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
        "  \"\"\"\n",
        "\n",
        "  assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
        "\n",
        "  ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "  # Save depth/dimension of the query embedding for scaling down the dot product\n",
        "  depth = query.shape[-1]\n",
        "\n",
        "  # Calculate scaled query key dot product according to formula above\n",
        "  dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth)\n",
        "\n",
        "  # Apply the mask\n",
        "  if mask is not None:   # The 'None' in this line does not need to be replaced\n",
        "    dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n",
        "\n",
        "  # Softmax formula implementation\n",
        "  # Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers\n",
        "  # Hint: Last axis should be used and keepdims should be True\n",
        "  # Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)\n",
        "  logsumexp = trax.fastmath.logsumexp(dots, axis=-1, keepdims=True)\n",
        "\n",
        "  # Take exponential of dots minus logsumexp to get softmax\n",
        "  # Use jnp.exp()\n",
        "  dots = jnp.exp(dots - logsumexp)\n",
        "\n",
        "  # Multiply dots by value to get self-attention\n",
        "  # Use jnp.matmul()\n",
        "  attention = jnp.matmul(dots, value)\n",
        "\n",
        "  ## END CODE HERE ###\n",
        "\n",
        "  return attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFIFoL6NTiqa"
      },
      "source": [
        "DotProductAttention(q_with_batch, k_with_batch, v_with_batch, m_bool)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FIKPI8-U5cU"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "DeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],\n",
        "              [1.        , 0.        , 1.        ]]], dtype=float32)\n",
        "```    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T_NKrUhWZxD"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "\n",
        "### 2.2 Causal Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqCy030WWa5s"
      },
      "source": [
        "Now you are going to implement causal attention: multi-headed attention with a mask to attend only to words that occurred before. \n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deeplearning.ai-NLPS/causal.png?raw=1' width='800'/>\n",
        "\n",
        "In the image above, a word can see everything that is before it, but not what is after it. To implement causal attention, you will have to transform vectors and do many reshapes. You will need to implement the functions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtMwUGjiWqWT"
      },
      "source": [
        "<a name='ex02'></a>\n",
        "#### Exercise 02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My6akHp5Wr3R"
      },
      "source": [
        "Implement the following functions that will be needed for Causal Attention:\n",
        "\n",
        "- <span style='color:blue'>**compute_attention_heads**</span>: Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\\times$ n_heads, seqlen, d_head).\n",
        "- <span style='color:blue'>**dot_product_self_attention**</span>: Creates a mask matrix with `False` values above the diagonal and `True` values below and calls DotProductAttention which implements dot product self attention.\n",
        "- <span style='color:blue'>**compute_attention_output**</span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads $\\times$ d_head). These operations concatenate (stack/merge) the heads. \n",
        "\n",
        "Next there are some toy tensors which may serve to give you an idea of the data shapes and opperations involved in Causal Attention. They are also useful to test out your functions! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whGbRAZaTpRS"
      },
      "source": [
        "tensor2d = create_tensor(q)\n",
        "display_tensor(tensor2d, \"query matrix (2D tensor)\")\n",
        "\n",
        "tensor4d2b = create_tensor([[q, q], [q, q]])\n",
        "display_tensor(tensor4d2b, \"batch of two (multi-head) collections of query matrices (4D tensor)\")\n",
        "\n",
        "tensor3dc = create_tensor([jnp.concatenate([q, q], axis=-1)])\n",
        "display_tensor(tensor3dc, \"one batch of concatenated heads of query matrices (3d tensor)\")\n",
        "\n",
        "tensor3dc3b = create_tensor([jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1)])\n",
        "display_tensor(tensor3dc3b, \"three batches of concatenated heads of query matrices (3d tensor)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFAjGcFYaU0H"
      },
      "source": [
        "It is important to know that the following 3 functions would normally be defined within the `CausalAttention` function further below. \n",
        "\n",
        "However this makes these functions harder to test. Because of this, these functions are shown individually using a `closure` (when necessary) that simulates them being inside of the `CausalAttention` function. This is done because they rely on some variables that can be accessed from within `CausalAttention`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNfI1iAYanF3"
      },
      "source": [
        "#### Support Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4BP09Vaao-f"
      },
      "source": [
        "<span style='color:blue'>**compute_attention_heads**</span>: Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\\times$ n_heads, seqlen, d_head).\n",
        "\n",
        "**For the closures you only have to fill the inner function.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZjO94ZtawKT"
      },
      "source": [
        "# UNQ_C2\n",
        "# GRADED FUNCTION: compute_attention_heads_closure\n",
        "def compute_attention_heads_closure(n_heads, d_head):\n",
        "  \"\"\"\n",
        "  Function that simulates environment inside CausalAttention function.\n",
        "  Args:\n",
        "      d_head (int):  dimensionality of heads.\n",
        "      n_heads (int): number of attention heads.\n",
        "  Returns:\n",
        "      function: compute_attention_heads function\n",
        "  \"\"\"\n",
        "\n",
        "  def compute_attention_heads(x):\n",
        "    \"\"\"\n",
        "    Compute the attention heads.\n",
        "    Args:\n",
        "        x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
        "    Returns:\n",
        "        jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
        "    \"\"\"\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "    # Size of the x's batch dimension\n",
        "    batch_size = x.shape[0]\n",
        "    # Length of the sequence\n",
        "    # Should be size of x's first dimension without counting the batch dim\n",
        "    seqlen = x.shape[1]\n",
        "\n",
        "    # Reshape x using jnp.reshape()\n",
        "    # batch_size, seqlen, n_heads*d_head -> batch_size, seqlen, n_heads, d_head\n",
        "    x = jnp.reshape(x, (batch_size, seqlen, n_heads, d_head))\n",
        "\n",
        "    # Transpose x using jnp.transpose()\n",
        "    # batch_size, seqlen, n_heads, d_head -> batch_size, n_heads, seqlen, d_head\n",
        "    # Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them\n",
        "    x = jnp.transpose(x, (0, 2, 1, 3))\n",
        "\n",
        "    # Reshape x using jnp.reshape()\n",
        "    # batch_size, n_heads, seqlen, d_head -> batch_size*n_heads, seqlen, d_head\n",
        "    x = jnp.reshape(x, (-1, seqlen, d_head))\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return x\n",
        "\n",
        "  return compute_attention_heads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wiamdms8VNo7"
      },
      "source": [
        "display_tensor(tensor3dc3b, \"input tensor\")\n",
        "result_cah = compute_attention_heads_closure(2, 3)(tensor3dc3b)\n",
        "display_tensor(result_cah, \"output tensor\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phop_O0vV5IU"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "input tensor shape: (3, 2, 6)\n",
        "\n",
        "[[[1 0 0 1 0 0]\n",
        "  [0 1 0 0 1 0]]\n",
        "\n",
        " [[1 0 0 1 0 0]\n",
        "  [0 1 0 0 1 0]]\n",
        "\n",
        " [[1 0 0 1 0 0]\n",
        "  [0 1 0 0 1 0]]]\n",
        "\n",
        "output tensor shape: (6, 2, 3)\n",
        "\n",
        "[[[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]]\n",
        "```\n",
        "\n",
        "<span style='color:blue'> **dot_product_self_attention** </span>: Creates a mask matrix with `False` values above the diagonal and `True` values below and calls DotProductAttention which implements dot product self attention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKq6H7HFVmnw"
      },
      "source": [
        "# UNQ_C3\n",
        "# GRADED FUNCTION: dot_product_self_attention\n",
        "def dot_product_self_attention(q, k, v):\n",
        "  \"\"\"\n",
        "  Masked dot product self attention.\n",
        "  Args:\n",
        "      q (jax.interpreters.xla.DeviceArray): queries.\n",
        "      k (jax.interpreters.xla.DeviceArray): keys.\n",
        "      v (jax.interpreters.xla.DeviceArray): values.\n",
        "  Returns:\n",
        "      jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.\n",
        "  \"\"\"\n",
        "\n",
        "  ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "  # Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)\n",
        "  # NOTE: there is a revision underway with the autograder to tolerate better indexing. \n",
        "  # Until then, please index q.shape using negative values (this is equivalent to counting from right to left)\n",
        "  mask_size = q.size(-2)\n",
        "\n",
        "  # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
        "  # Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_\n",
        "  # Use jnp.tril() - Lower triangle of an array and jnp.ones()\n",
        "  mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
        "\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return DotProductAttention(q, k, v, mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpVOhvh6FVlT"
      },
      "source": [
        "dot_product_self_attention(q_with_batch, k_with_batch, v_with_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3tMQ2DIFbi8"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "DeviceArray([[[0.        , 1.        , 0.        ],\n",
        "              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)\n",
        "```\n",
        "\n",
        "<span style='color:blue'> **compute_attention_output** </span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads $\\times$ d_head). These operations concatenate (stack/merge) the heads. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cs7fM2bFi9I"
      },
      "source": [
        "# UNQ_C4\n",
        "# GRADED FUNCTION: compute_attention_output_closure\n",
        "def compute_attention_output_closure(n_heads, d_head):\n",
        "  \"\"\"\n",
        "  Function that simulates environment inside CausalAttention function.\n",
        "  Args:\n",
        "      d_head (int):  dimensionality of heads.\n",
        "      n_heads (int): number of attention heads.\n",
        "  Returns:\n",
        "      function: compute_attention_output function\n",
        "  \"\"\"\n",
        "\n",
        "  def compute_attention_output(x):\n",
        "    \"\"\"\n",
        "    Compute the attention output.\n",
        "    Args:\n",
        "        x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
        "    Returns:\n",
        "        jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
        "    \"\"\"\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "        \n",
        "    # Length of the sequence\n",
        "    # Should be size of x's first dimension without counting the batch dim\n",
        "    seqlen = x.shape[1]\n",
        "\n",
        "    # Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)\n",
        "    x = jnp.reshape(x, (-1, n_heads, d_head))\n",
        "\n",
        "    # Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)\n",
        "    x = jnp.transpose(x, (0, 2, 1, 3))\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Reshape to allow to concatenate the heads\n",
        "\n",
        "    return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
        "\n",
        "  return compute_attention_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-pXvtdwHMEc"
      },
      "source": [
        "display_tensor(result_cah, \"input tensor\")\n",
        "result_cao = compute_attention_output_closure(2,3)(result_cah)\n",
        "display_tensor(result_cao, \"output tensor\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LppqTunOHQtK"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "input tensor shape: (6, 2, 3)\n",
        "\n",
        "[[[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]\n",
        "\n",
        " [[1 0 0]\n",
        "  [0 1 0]]]\n",
        "\n",
        "output tensor shape: (3, 2, 6)\n",
        "\n",
        "[[[1 0 0 1 0 0]\n",
        "  [0 1 0 0 1 0]]\n",
        "\n",
        " [[1 0 0 1 0 0]\n",
        "  [0 1 0 0 1 0]]\n",
        "\n",
        " [[1 0 0 1 0 0]\n",
        "  [0 1 0 0 1 0]]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r25tZntrHRNU"
      },
      "source": [
        "#### Causal Attention Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqvtOGjIHTOi"
      },
      "source": [
        "Now it is time for you to put everything together within the `CausalAttention` or Masked multi-head attention function:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deeplearning.ai-NLPS/masked-attention.png?raw=1' width='800'/>\n",
        "\n",
        "**Instructions:** Implement the causal attention.\n",
        "Your model returns the causal attention through a $tl.Serial$ with the following:\n",
        "\n",
        "- <span style='color:blue'> [tl.Branch](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch) </span>: consisting of 3 [tl.Dense(d_feature), ComputeAttentionHeads] to account for the queries, keys, and values.\n",
        "- <span style='color:blue'> [tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn)</span>: Takes in dot_product_self_attention function and uses it to compute the dot product using $Q$, $K$, $V$.\n",
        "- <span style='color:blue'> [tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn)</span>: Takes in restack_attention_heads to allow for parallel computing.\n",
        "- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense)</span>: Final Dense layer, with dimension `d_feature`.\n",
        "\n",
        "Remember that in order for trax to properly handle the functions you just defined, they need to be added as layers using the [`tl.Fn()`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn) function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY7zKhqVHxl-"
      },
      "source": [
        "# UNQ_C5\n",
        "# GRADED FUNCTION: CausalAttention\n",
        "def CausalAttention(d_feature,\n",
        "                    n_heads,\n",
        "                    compute_attention_heads_closure=compute_attention_heads_closure,\n",
        "                    dot_product_self_attention=dot_product_self_attention,\n",
        "                    compute_attention_output_closure=compute_attention_output_closure,\n",
        "                    mode=\"train\"):\n",
        "  \"\"\"\n",
        "  Transformer-style multi-headed causal attention.\n",
        "\n",
        "  Args:\n",
        "      d_feature (int):  dimensionality of feature embedding.\n",
        "      n_heads (int): number of attention heads.\n",
        "      compute_attention_heads_closure (function): Closure around compute_attention heads.\n",
        "      dot_product_self_attention (function): dot_product_self_attention function. \n",
        "      compute_attention_output_closure (function): Closure around compute_attention_output. \n",
        "      mode (str): 'train' or 'eval'.\n",
        "\n",
        "  Returns:\n",
        "      trax.layers.combinators.Serial: Multi-headed self-attention model.\n",
        "  \"\"\"\n",
        "\n",
        "  assert d_feature % n_heads == 0\n",
        "  d_head = d_feature // n_heads\n",
        "\n",
        "  ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "  # HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)\n",
        "  # Since you are dealing with closures you might need to call the outer \n",
        "  # function with the correct parameters to get the actual uncalled function.\n",
        "  ComputeAttentionHeads = tl.Fn(\"AttnHeads\", compute_attention_heads_closure(n_heads, d_head), n_out=1)\n",
        "\n",
        "  return tl.Serial(\n",
        "      tl.Branch(  # creates three towers for one input, takes activations and creates queries keys and values\n",
        "          [tl.Dense(d_feature), ComputeAttentionHeads], # queries\n",
        "          [tl.Dense(d_feature), ComputeAttentionHeads], # keys\n",
        "          [tl.Dense(d_feature), ComputeAttentionHeads], # values\n",
        "      ),\n",
        "      tl.Fn(\"DotProductAttn\", dot_product_self_attention, n_out=1),  # takes QKV\n",
        "      # HINT: The second argument to tl.Fn() is an uncalled function\n",
        "      # Since you are dealing with closures you might need to call the outer \n",
        "      # function with the correct parameters to get the actual uncalled function.\n",
        "      tl.Fn(\"AttnOutput\", compute_attention_output_closure(n_heads, d_head), n_out=1), # to allow for parallel\n",
        "      tl.Dense(d_feature)   # Final dense layer\n",
        "  )\n",
        "\n",
        "  ### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSY6TM018xED"
      },
      "source": [
        "# Take a look at the causal attention model\n",
        "print(CausalAttention(d_feature=512, n_heads=8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l-MQmxE88oM"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "Serial[\n",
        "  Branch_out3[\n",
        "    [Dense_512, AttnHeads]\n",
        "    [Dense_512, AttnHeads]\n",
        "    [Dense_512, AttnHeads]\n",
        "  ]\n",
        "  DotProductAttn_in3\n",
        "  AttnOutput\n",
        "  Dense_512\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlKh_Vxh89LP"
      },
      "source": [
        "<a name='2.3'></a>\n",
        "### 2.3 Transformer decoder block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LMZgTfh9Aar"
      },
      "source": [
        "Now that you have implemented the causal part of the transformer, you will implement the transformer decoder block. Concretely you will be implementing this image now.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deeplearning.ai-NLPS/transformer_decoder_1.png?raw=1' width='800'/>\n",
        "\n",
        "To implement this function, you will have to call the `CausalAttention` or Masked multi-head attention function you implemented above. You will have to add a feedforward which consists of: \n",
        "\n",
        "- <span style='color:blue'> [tl.LayerNorm](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm) </span>: used to layer normalize\n",
        "- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: the dense layer\n",
        "- <span style='color:blue'> [ff_activation](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu) </span>: feed forward activation (we use ReLu) here.\n",
        "- <span style='color:blue'> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout) </span>: dropout layer\n",
        "- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: dense layer\n",
        "- <span style='color:blue'> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout) </span>: dropout layer\n",
        "\n",
        "Finally once you implement the feedforward, you can go ahead and implement the entire block using: \n",
        "\n",
        "- <span style='color:blue'> [tl.Residual](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual) </span>: takes in the tl.LayerNorm(), causal attention block, tl.dropout. \n",
        "\n",
        "- <span style='color:blue'> [tl.Residual](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual) </span>: takes in the feedforward block you will implement. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDRNFgUg9zEN"
      },
      "source": [
        "<a name='ex03'></a>\n",
        "#### Exercise 03\n",
        "**Instructions:** Implement the transformer decoder block. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgwEcwsC9QSJ"
      },
      "source": [
        "# UNQ_C6\n",
        "# GRADED FUNCTION: DecoderBlock\n",
        "def DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation):\n",
        "  \"\"\"\n",
        "  Returns a list of layers that implements a Transformer decoder block.\n",
        "\n",
        "  The input is an activation tensor.\n",
        "\n",
        "  Args:\n",
        "      d_model (int):  depth of embedding.\n",
        "      d_ff (int): depth of feed-forward layer.\n",
        "      n_heads (int): number of attention heads.\n",
        "      dropout (float): dropout rate (how much to drop out).\n",
        "      mode (str): 'train' or 'eval'.\n",
        "      ff_activation (function): the non-linearity in feed-forward layer.\n",
        "\n",
        "  Returns:\n",
        "      list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
        "  \"\"\"\n",
        "\n",
        "  ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "  # Create masked multi-head attention block using CausalAttention function\n",
        "  causal_attention = CausalAttention(d_model, n_heads=n_heads, mode=mode)\n",
        "\n",
        "  # Create feed-forward block (list) with two dense layers with dropout and input normalized\n",
        "  feed_forward = [\n",
        "      # Normalize layer inputs\n",
        "      tl.LayerNorm(),\n",
        "      # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n",
        "      tl.Dense(d_ff),\n",
        "      # Add activation function passed in as a parameter (you need to call it!)\n",
        "      ff_activation(),  # Generally ReLU\n",
        "      # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
        "      tl.Dropout(rate=dropout, mode=mode),\n",
        "      # Add second feed forward layer (don't forget to set the correct value for n_units)\n",
        "      tl.Dense(d_model),\n",
        "      # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
        "      tl.Dropout(rate=dropout, mode=mode)            \n",
        "  ]\n",
        "\n",
        "  # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n",
        "  return [\n",
        "     tl.Residual(\n",
        "         # Normalize layer input\n",
        "         tl.LayerNorm(),\n",
        "         # Add causal attention block previously defined (without parentheses)\n",
        "         causal_attention,\n",
        "         # Add dropout with rate and mode specified\n",
        "         tl.Dropout(rate=dropout, mode=mode)\n",
        "     ),\n",
        "     tl.Residual(\n",
        "         # Add feed forward block (without parentheses)\n",
        "         feed_forward\n",
        "     ),    \n",
        "  ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t88ObmMK-FnD"
      },
      "source": [
        "# Take a look at the decoder block\n",
        "print(DecoderBlock(d_model=512, d_ff=2048, n_heads=8, dropout=0.1, mode='train', ff_activation=tl.Relu))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRLPvXGlCUmW"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "[Serial[\n",
        "  Branch_out2[\n",
        "    None\n",
        "    Serial[\n",
        "      LayerNorm\n",
        "      Serial[\n",
        "        Branch_out3[\n",
        "          [Dense_512, AttnHeads]\n",
        "          [Dense_512, AttnHeads]\n",
        "          [Dense_512, AttnHeads]\n",
        "        ]\n",
        "        DotProductAttn_in3\n",
        "        AttnOutput\n",
        "        Dense_512\n",
        "      ]\n",
        "      Dropout\n",
        "    ]\n",
        "  ]\n",
        "  Add_in2\n",
        "], Serial[\n",
        "  Branch_out2[\n",
        "    None\n",
        "    Serial[\n",
        "      LayerNorm\n",
        "      Dense_2048\n",
        "      Relu\n",
        "      Dropout\n",
        "      Dense_512\n",
        "      Dropout\n",
        "    ]\n",
        "  ]\n",
        "  Add_in2\n",
        "]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbOTc67sCZv-"
      },
      "source": [
        "<a name='2.4'></a>\n",
        "### 2.4 Transformer Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JVw1WGiCdUT"
      },
      "source": [
        "You will now bring it all together. In this part you will use all the subcomponents you previously built to make the final model. Concretely, here is the image you will be implementing. \n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deeplearning.ai-NLPS/transformer_decoder.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulu8z3jtCpg1"
      },
      "source": [
        "<a name='ex04'></a>\n",
        "#### Exercise 04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOXd_4O5CqxH"
      },
      "source": [
        "**Instructions:** Previously you coded the decoder block. Now you will code the transformer language model. Here is what you will need. \n",
        "\n",
        "- <span style=\"color:blue\"> positional_enconder </span>- a list containing the following layers:\n",
        "    - <span style=\"color:blue\"> [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding)\n",
        "    - <span style=\"color:blue\"> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout)\n",
        "    - <span style=\"color:blue\"> [tl.PositionalEncoding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PositionalEncoding)\n",
        "\n",
        "- A list of `n_layers` <span style=\"color:blue\"> decoder blocks</span>.\n",
        "- <span style=\"color:blue\"> [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial): </span> takes in the following layers or lists of layers:\n",
        "    - <span style=\"color:blue\"> [tl.ShiftRight](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight): </span>: shift the tensor to the right by padding on axis 1.\n",
        "    - <span style=\"color:blue\"> positional_encoder </span>: encodes the text positions.\n",
        "    - <span style=\"color:blue\"> decoder_blocks </span>: the ones you created.\n",
        "    - <span style=\"color:blue\"> [tl.LayerNorm](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm) </span>: a layer norm.\n",
        "    - <span style=\"color:blue\"> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: takes in the vocab_size.\n",
        "    - <span style=\"color:blue\"> [tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax) </span>: to predict.\n",
        "    \n",
        "Go go go!! You can do it :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLKVaJ--Cw1i"
      },
      "source": [
        "# UNQ_C7\n",
        "# GRADED FUNCTION: TransformerLM\n",
        "def TransformerLM(vocab_size=33300, d_model=512, d_ff=2048, n_layers=6, n_heads=8, dropout=0.1, max_len=4096, mode=\"train\", ff_activation=tl.Relu):\n",
        "  \"\"\"\n",
        "  Returns a Transformer language model.\n",
        "\n",
        "  The input to the model is a tensor of tokens. (This model uses only the\n",
        "  decoder part of the overall Transformer.)\n",
        "\n",
        "  Args:\n",
        "      vocab_size (int): vocab size.\n",
        "      d_model (int):  depth of embedding.\n",
        "      d_ff (int): depth of feed-forward layer.\n",
        "      n_layers (int): number of decoder layers.\n",
        "      n_heads (int): number of attention heads.\n",
        "      dropout (float): dropout rate (how much to drop out).\n",
        "      max_len (int): maximum symbol length for positional encoding.\n",
        "      mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n",
        "      ff_activation (function): the non-linearity in feed-forward layer.\n",
        "\n",
        "  Returns:\n",
        "      trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens to activations over a vocab set.\n",
        "  \"\"\"\n",
        "\n",
        "  ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "  # Embedding inputs and positional encoder\n",
        "  positional_encoder = [\n",
        "     # Add embedding layer of dimension (vocab_size, d_model)\n",
        "     tl.Embedding(vocab_size, d_model),\n",
        "     # Use dropout with rate and mode specified\n",
        "     tl.Dropout(rate=dropout, mode=mode),\n",
        "     # Add positional encoding layer with maximum input length and mode specified\n",
        "     tl.PositionalEncoding(max_len=max_len, mode=mode)                   \n",
        "  ]\n",
        "\n",
        "  # Create stack (list) of decoder blocks with n_layers with necessary parameters\n",
        "  decoder_blocks = [\n",
        "     DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)               \n",
        "  ]\n",
        "\n",
        "  # Create the complete model as written in the figure\n",
        "  return tl.Serial(\n",
        "      # Use teacher forcing (feed output of previous step to current step)\n",
        "      tl.ShiftRight(mode=mode),   # Specify the mode!\n",
        "      # Add positional encoder\n",
        "      positional_encoder,\n",
        "      # Add decoder blocks\n",
        "      decoder_blocks,\n",
        "      # Normalize layer\n",
        "      tl.LayerNorm(),\n",
        "\n",
        "      # Add dense layer of vocab_size (since need to select a word to translate to)\n",
        "      # (a.k.a., logits layer. Note: activation already set by ff_activation)\n",
        "      tl.Dense(vocab_size),\n",
        "      # Get probabilities with Logsoftmax\n",
        "      tl.LogSoftmax(),\n",
        "  )\n",
        "\n",
        "  ### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7S5arCFHMoc"
      },
      "source": [
        "# Take a look at the Transformer\n",
        "print(TransformerLM(n_layers=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jOM6eV4HfNF"
      },
      "source": [
        "**Expected Output:**\n",
        "```CPP\n",
        "Serial[\n",
        "  ShiftRight(1)\n",
        "  Embedding_33300_512\n",
        "  Dropout\n",
        "  PositionalEncoding\n",
        "  Serial[\n",
        "    Branch_out2[\n",
        "      None\n",
        "      Serial[\n",
        "        LayerNorm\n",
        "        Serial[\n",
        "          Branch_out3[\n",
        "            [Dense_512, AttnHeads]\n",
        "            [Dense_512, AttnHeads]\n",
        "            [Dense_512, AttnHeads]\n",
        "          ]\n",
        "          DotProductAttn_in3\n",
        "          AttnOutput\n",
        "          Dense_512\n",
        "        ]\n",
        "        Dropout\n",
        "      ]\n",
        "    ]\n",
        "    Add_in2\n",
        "  ]\n",
        "  Serial[\n",
        "    Branch_out2[\n",
        "      None\n",
        "      Serial[\n",
        "        LayerNorm\n",
        "        Dense_2048\n",
        "        Relu\n",
        "        Dropout\n",
        "        Dense_512\n",
        "        Dropout\n",
        "      ]\n",
        "    ]\n",
        "    Add_in2\n",
        "  ]\n",
        "  LayerNorm\n",
        "  Dense_33300\n",
        "  LogSoftmax\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ0EpWR0HiJS"
      },
      "source": [
        "<a name='3'></a>\n",
        "## Part 3: Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKuDKDlnHkzo"
      },
      "source": [
        "Now you are going to train your model. As usual, you have to define the cost function, the optimizer, and decide whether you will be training it on a `gpu` or `cpu`. In this case, you will train your model on a cpu for a few steps and we will load in a pre-trained model that you can use to predict with your own words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neBNEbdPHnop"
      },
      "source": [
        "<a name='3.1'></a>\n",
        "### 3.1 Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZcVD4ZqHqID"
      },
      "source": [
        "You will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set. Each iteration is defined as an `epoch`. For each epoch, you have to go over all the data, using your training iterator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nCkjPULHsUq"
      },
      "source": [
        "<a name='ex05'></a>\n",
        "#### Exercise 05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dmZ7tp6HvyH"
      },
      "source": [
        "**Instructions:** Implement the `train_model` program below to train the neural network above. Here is a list of things you should do:\n",
        "\n",
        "- Create the train task by calling [`trax.supervised.training.TrainTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) and pass in the following: \n",
        "    - <span style='color:blue'> labeled_data </span> = train_gen\n",
        "    - <span style='color:blue'> loss_fn </span> = [tl.CrossEntropyLoss()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss)\n",
        "    - <span style='color:blue'> optimizer </span> = [trax.optimizers.Adam(0.01)](https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam)\n",
        "    - <span style='color:blue'> lr_schedule </span> = [lr_schedule](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.lr_schedules.warmup_and_rsqrt_decay)\n",
        "\n",
        "\n",
        "- Create the eval task by calling [`trax.supervised.training.EvalTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) and pass in the following: \n",
        "    - <span style='color:blue'> labeled_data </span> = eval_gen\n",
        "    - <span style='color:blue'> metrics </span> = tl.CrossEntropyLoss() and [tl.Accuracy()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy)\n",
        "    \n",
        "    \n",
        "- Create the training loop by calling [`trax.supervised.Training.Loop`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) and pass in the following: \n",
        "    - <span style='color:blue'> TransformerLM </span> \n",
        "    - <span style='color:blue'> train_task </span> \n",
        "    - <span style='color:blue'> eval_task </span> = [eval_task]\n",
        "    - <span style='color:blue'> output_dir</span> = output_dir\n",
        "    \n",
        "You will be using a cross entropy loss, with Adam optimizer. Please read the [Trax](https://trax-ml.readthedocs.io/en/latest/index.html) documentation to get a full understanding. \n",
        "\n",
        "The training loop that this function returns can be runned using the `run()` method by passing in the desired number of steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL1wnRTAHzNz"
      },
      "source": [
        "from trax.supervised import training\n",
        "\n",
        "# UNQ_C8\n",
        "# GRADED FUNCTION: train_model\n",
        "def training_loop(TransformerLM, train_gen, eval_gen, output_dir=\"~/model\"):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "      TransformerLM (trax.layers.combinators.Serial): The model you are building.\n",
        "      train_gen (generator): Training stream of data.\n",
        "      eval_gen (generator): Evaluation stream of data.\n",
        "      output_dir (str): folder to save your file.\n",
        "      \n",
        "  Returns:\n",
        "      trax.supervised.training.Loop: Training loop.\n",
        "  \"\"\"\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}